\chapter{Code Generation}

In the code generation stage, we use the type-annotated syntax tree from our
typing stage to generate code for the \emph{Simple Stack Machine}
(\emph{SSM})\footnote{\url{https://gitlab.science.ru.nl/compilerconstruction/ssm}}.
The typing information we inferred and checked during the previous stage is
vital to the code generation, as we need it to determine which data to store on
the heap and the stack, and how much space we need to allocate.
Of course, the semantic analyses also preclude many possible errors that might
arise during code generation, or at runtime, due to e.g. mismatched types,
missing return statements, or functions being called prior to their definition
in the source file.


\section{Evaluation Order}

We take the evaluation strategy of SPL to be \emph{eager}, or call-by-value,
meaning that function arguments are evaluated prior to performing function
application. For instance, in the function call \spl{f(1+1)}, we first evaluate
the expression \spl{1+1} to \spl{2} before performing any computation in the
\emph{body} (that is, the definition) of the function \spl{f}.
In more practical terms, this means that the generated SSM instructions will
first reduce all argument expressions, such that these are in value form on the
stack before we jump to the respective function.

As a consequence, we may perform unnecessary evaluation of arguments if they are
never used in the function body, though the upside is that values only require
a fixed amount of space (for the base types \spl{Int}, \spl{Bool}, and
\spl{Char} at least---we will talk about lists in detail in
\cref{sec:multi-reg-vals}), while a \emph{lazy} evaluation strategy would
require us to store unevaluated expressions, which may be arbitrarily large.
The memory usage under call-by-value is thus significantly smaller, and we avoid
complicated memory management involving the storing and loading of arbitrary
expressions.


\section{General Setup}

Our code generation stage makes use of the state monad, where the state consists
of a label counter which is simply a `fresh' integer we append to our labels to
ensure their uniqueness, along with two maps from \haskell{Text} to
\haskell{Int} for storing offsets on the stack for local variables, and heap
locations for global variables and boxed data.


\section{Instructions}

We defined algebraic data types \haskell{Register} and \haskell{Instr} for the
register names and instructions available in SSM. An SSM program is simply a
list of instructions: \haskell{type Program = [Instr]}.
By defining \haskell{Show} instances for the registers and instructions, we can
easily print/output a \verb|.ssm| file from an instance of type
\haskell{[Instr]} with \haskell{unlines $ show <$> p}.

An excerpt of the data type for instructions and the corresponding \haskell{Show}
instance is given below.

\begin{minted}{haskell}
  data Instr = Label T.Text | Ret | Halt | Link Int | Unlink | Adjust Int | ...

  instance Show Instr where
    show (Label t) = T.unpack t <> ":"
    show Ret = tab <> "ret"
    show Halt = tab <> "halt"
    show (Link i) = tab <> "link" <+> show i
    show Unlink = tab <> "unlink"
    show (Adjust i) = tab <> "ajs" <+> show i
    ...
\end{minted}

All instructions except for labels are indented by two spaces, which helps with
navigating the resulting SSM code.



\section{Multi-Register Values} \label{sec:multi-reg-vals}

While values of the base types $\Int$, $\Bool$ and $\Char$ can all be
stored in a single register, value instances of lists
require---in general---multiple registers.
We also support tuples, which also require at least two registers:
values of type \code{\TProd{$\tau_1$}{$\tau_2$}} can be stored in two registers
when both $\tau_1$ and $\tau_2$ are base types.
The left and right component may again be multi-register values though, and
the programmer can nest tuples to an arbitrary depth, with types such as
$\TProd{\TProd{\Int}{\Bool}}{\TProd{\Int}{\Char}}$. In this case, we would
require four registers, within which we must compute the offsets when
translating e.g. the selectors \code{.fst} and \code{.snd}.

Perhaps more importantly, values of a list type \code{[$\tau$]} may require
arbitrarily many registers, even when $\tau$ is a base type such as $\Int$.
We store lists in a \emph{linked} representation on the heap, that is, as a
number of list segments which reference each other's location in memory.
More specifically, the $i$-th element of a list is stored in two components: the
value at the $i$-th position, along with the address at which the subsequent
element of the list is stored.
To terminate such a chain of references, we reserve the register value
\code{0xF0F0F0F0} as a `null pointer', similar to e.g. \code{null} in C or Java.
We use this null pointer in the final segment of a list to express that the
second component does not refer to the next segment, but instead terminates the
list. In this way, the null pointer is the SSM counterpart to SPL's `nil', or
\spl{[]}.

\subsection{List Representation Trade-Offs}
At this point, it may seem odd that we use such a linked representation of
lists, as it is rather inefficient in both space and time complexity when
compared to storing the list elements in an array of fixed size.
However, such a representation has some important advantages when dealing with
polymorphism, which we will cover in \cref{sec:polymorphism-boxing}, and in
mapping the semantics of SPL onto the instruction set of SSM.
The semantics of lists in SPL is much like that of lists in functional,
high-level languages such as ML/OCaml or Haskell, where lists are viewed as
essentially left-deep nested tuples, decomposable into a \emph{head} (the first
element of the list) and a \emph{tail}, the remainder of the list following the
head. This is reflected in the way SPL allows construction of lists, that is,
via the \emph{cons} constructor `\spl{:}' with type
$\forall \alpha.\ \tau \to \TList{\alpha} \to \TList{\alpha}$, as well as the
destructors (selectors) on lists, namely \spl{.hd} and \spl{.tl}, which take a
non-nil list instance and return the head and tail, respectively.

Such a recursive representation of lists stands in contrast to the \emph{arrays}
that are a staple of imperative languages such as C or Java. These correspond
to a block of registers with fixed size, where the registers can be
accessed via indexing in $\bigO(1)$ time.
Importantly, the size of an array is fixed at instantiation and thus known
a-priori, while a recursive SPL list can grow and shrink at runtime. To express
such a semantics in SPL while storing the list elements in register blocks would
require an abstraction mechanism similar to e.g. Java's
\mintinline{java}|ArrayList<T>|, where the list is represented internally as an
array, along with the largest index currently in use. The internal array is then
upsized and downsized dynamically as necessary (or when reasonable).

Employing such a strategy in the code generation would result in significantly
faster access times for lists, since accesses are $\bigO(1)$ for array lists
too, compared to $\bigO(n)$ for linked lists.
Despite this, we chose the linked representation in SSM for the following reasons:
%
\begin{itemize}
  \item To benefit from the fast random access of arrays, SPL would need to be
        extended with a construct for index-based lookup in lists. With only the
        \spl{.hd} and \spl{.tl} selectors, the programmer could not leverage the
        improved access times in the first place.
  %
  \item Generating code for \code{ArrayList}-style lists would be a lot more
        complicated, especially in their interaction with polymorphism, and due
        to the fact that the size of lists in SPL cannot be determined
        statically. For instance, the length of a list may depend on user input,
        and thus our generated code would need to determine at runtime when an
        (array) list needs to be resized. At that point, it would make more
        sense to support an array (or rather, \emph{vector}) type in SPL
        directly, and then implement an \code{ArrayList}-style abstract
        interface within SPL, rather than in the compiler.
  %
  \item For lists with relatively few elements, the performance advantages of
        arrays are negligible, and even more so for lists which are frequently
        modified by appending or removing elements at the start of the list,
        which may even be slower with a dynamically resized array.
        These are common use cases in SPL, and so again, it would seem more
        reasonable to support a specific vector type for use cases where (very)
        large lists must be accessed often and out-of-order.
  %
  \item Using a representation of lists in SSM that is fundamentally different
        to the lists of SPL would lead to rather unpredictable performance from
        the user perspective. For example, extending a list
        $\code{l}$ of type $\TList{\Int}$ by \spl{l := 1:l;} could occasionally
        require copying all elements in \code{l}, causing the same operation to
        vary between being near-instant and very slow, which seems undesirable
        from the perspective of the programmer.
\end{itemize}

Overall, we think that it would be more worthwhile to extend SPL with a vector
type distinct from the list type $\TList{\tau}$, as well as syntax for
index-based lookup in a vector, e.g. with $\code{.(}i\code{)}$ as a new
selector, where $i$ is an expression of type $\Int$.


\subsection{Polymorphism and Boxing} \label{sec:polymorphism-boxing}

As pointed out in the previous section, our choice of representation for lists
and tuples in SSM is also informed by the code generation for polymorphic
functions, and their interaction with the data on which they operate.

To a simple example (perhaps the simplest), let us consider the polymorphic
identity function:
\begin{lstlisting}[language=spl]
  id(x) :: a -> a {
    return x;
  }
\end{lstlisting}
%
Throughout a given source program, this function may be called at a whole number
of different types, e.g. \spl{id(42)}, \spl{id(42:[])}, or \spl{id((40,2))}. For
translating such a function to SSM, we have two general approaches we can choose
between:
\begin{enumerate}[label={(\arabic*)}]
  \item \textbf{Monomorphisation.} We could generate a monomorphic instance of
        the \spl{id} function for each type at which the function is called. For
        the example calls above, we would then need instances for $\Int$,
        $\TList{\Int}$, as well as $\TProd{\Int}{\Int}$.
        Such an approach would allow us to use e.g. a multi-register
        representation of tuples on the stack. Since we generate a specific
        instance of \spl{id} for the tuple type $\TProd{\Int}{\Int}$, the
        monomorphic instance can account for the tuple argument spanning
        multiple registers, and handle it accordingly.
  %
  \item \textbf{Boxing.} Alternatively, we can choose to represent lists and
        tuples such that they can be treated uniformly with the single-register
        base types $\Int$, $\Bool$ and $\Char$.
        This approach involves \emph{boxing} all tuple and list values, that is,
        storing them entirely on the heap, and passing the location on the heap
        to the function as the argument.
        As a result, we can treat all types uniformly, and rather than
        generating a whole host of specialised monomorphic instances, we require
        only one instance of the \spl{id} function in SSM, which simply returns
        the value in the register for the first argument, without needing to
        consider the argument type in any way.
\end{enumerate}

One downside of (2) is that boxing tuples and lists results in a slight
performance decrease whenever we actually construct or destruct tuple and list
instances. In particular, instances of these types are always pointers to the
heap, so we must always first load from the heap when accessing the data.

A larger disadvantage of storing all tuples and lists on the heap is an increase
in memory usage. For instance, we might call a function which constructs and
returns a nested tuple value from which we select just one of the entries and
discard the rest.
In such a scenario, the entire tuple remains stored on the heap, and freeing the
occupied registers would require us to implement some form of memory management,
that is, moving around entries on the heap such that we can decrement the heap
pointer and free up registers whose data is no longer needed.

On the other hand, an issue with the monomorphisation approach of (1) is that it
may lead to a combinatorial explosion of specialised function instances.
Take, for example, the following function, which converts left-deep tuples into
right-deep tuples:
\begin{lstlisting}[language=spl]
  renest(t) :: ((a,b),c) -> (a,(b,c)) {
    var t1 = t.fst;
    return (t1.fst, (t1.snd, t.snd));
  }
\end{lstlisting}
%
The programmer may define such a polymorphic function and use it with numerous
unique combinations of types for the type variables \spl{a}, \spl{b} and
\spl{c}. If each type variable is instantiated with e.g. three different types,
we may already need up to 27 monomorphic function instances to cover all calls
to the \spl{renest} function.
We could of course mitigate this combinatorial increase somewhat by e.g.
treating at least all single-register base types uniformly, though even then
there would still be cases where we get many specialised instances, albeit for
slightly more contrived examples.


\subsection{Copy Semantics of Lists and Tuples}
Beyond the considerations regarding the representation of lists and tuples
covered so far, we must also pay attention to how our choices may
(inadvertently) affect the semantics of lists and tuples.
If we chose the monomorphisation approach for tuples and allocated all tuples on
the stack, then loading a tuple from e.g. a local or global variable would
result in a full copy, i.e. the entries (fields) are copied to
the top of the stack.

Consider the following function, which allocates a tuple in a local variable,
which it then passes to the function \spl{g}.
%
\begin{lstlisting}[language=spl]
  f(x) :: Int -> Int {
    (Int,Bool) t = (x,True);
    g(t);
    return t.fst;
  }

  g(t) :: (Int,a) -> Void {
    t.fst = t.fst + 1;
    return;
  }
\end{lstlisting}
%
In the first approach, loading a tuple results in both fields being placed on
the stack, such that \spl{g} is called with the \emph{data} of the tuple.
Modifications to the tuple performed in \spl{g} are thus not reflected in \spl{f}. Here, \spl{g} increments the first entry of its argument,
but \spl{g} is operating on a copy, while in the scope of \spl{f}, the variable
\spl{t} refers to the original tuple data.
As such, the function call \spl{g(t)} in \spl{f} cannot affect the output of
\spl{f}. This behaviour is commonly referred to as \emph{pass-by-value}, since
the \emph{value} of \spl{t} (in this case, the $\Int$ and $\Bool$ field) is
passed to the function.

Pass-by-value stands in contrast to a \emph{pass-by-reference} semantics, where
\spl{g} instead receives a reference to \spl{t}, that is, the location of the
original data, rather than a copy.
We obtain such a semantics with the boxing approach, since the local variable
\spl{t} then stores the heap location of the tuple data. In the call \spl{g(t)},
the \emph{location} of \spl{t} is passed to \spl{g}, rather than the data itself,
and the incrementation performed in \spl{g} will thus also be reflected in the
scope of \spl{f}, since the original data on the heap is modified.
For example, the following \spl{main} function would print \spl{43} with
pass-by-reference, while for pass-by-value semantics it would print \spl{42}.
%
\begin{lstlisting}[language=spl]
  main() {
    print(f(42));
  }
\end{lstlisting}



\begin{itemize}
	\item Compilation scheme?
	\item How is data represented? Lists, tuples
	\item Semantics style, call-by-reference, call-by-value?
	\item How did you solve overloaded functions?
	\item Polymorphism?
	\item Printing?
	\item Problems?
	\item\ldots
\end{itemize}
