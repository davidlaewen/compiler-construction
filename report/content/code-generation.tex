\chapter{Code Generation}

In the code generation stage, we use the type-annotated syntax tree from our
typing stage to generate code for the \emph{Simple Stack Machine}
(\emph{SSM})\footnote{\url{https://gitlab.science.ru.nl/compilerconstruction/ssm}}.
The typing information we inferred and checked during the previous stage is
vital to the code generation, as we need it to determine which data to store on
the heap and the stack, and how much space we need to allocate.
Of course, the semantic analyses also preclude many possible errors that might
arise during code generation, or at runtime, due to e.g. mismatched types,
missing return statements, or functions being called prior to their definition
in the source file.


\section{Evaluation Order}

We take the evaluation strategy of SPL to be \emph{eager}, or call-by-value,
meaning that function arguments are evaluated prior to performing function
application. For instance, in the function call \spl{f(1+1)}, we first evaluate
the expression \spl{1+1} to \spl{2} before performing any computation in the
\emph{body} (that is, the definition) of the function \spl{f}.
In more practical terms, this means that the generated SSM instructions will
first reduce all argument expressions, such that these are in value form on the
stack before we jump to the respective function.

As a consequence, we may perform unnecessary evaluation of arguments if they are
never used in the function body, though the upside is that values only require
a fixed amount of space (for the base types \spl{Int}, \spl{Bool}, and
\spl{Char} at least---we will talk about lists in detail in
\cref{sec:multi-reg-vals}), while a \emph{lazy} evaluation strategy would
require us to store unevaluated expressions, which may be arbitrarily large.
The memory usage under call-by-value is thus significantly smaller, and we avoid
complicated memory management involving the storing and loading of arbitrary
expressions.



\section{Code Generation Monad} \label{sec:codegen-monad}

Our code generation stage makes use of the state monad, where the state consists
of a label counter of type \haskell{Int}, along with two maps from
\haskell{Text} to \haskell{Int}.
The label counter simply allows us to obtain `fresh', previously unused integers
by taking the current value and then incrementing the counter. We use the label
counter in the labels we generate for \spl{if} and \spl{while} statements, for
which we use labels `else', `endif', and so on, and then append a fresh integer
to the name to ensure that the label name is unique, and doesn't clash with the
labels generated for other parts of the code.

Of the two finite maps, the first one is for storing offsets on the stack for
local variables and function arguments, and the other for heap locations, i.e.
global variables.

\begin{minted}{haskell}
  type LocationMap = M.Map T.Text Int

  data CodegenState = CodegenState {
    labelCounter :: Int,
    offsets :: LocationMap,
    heapLocs :: LocationMap
  }

  type Codegen = State CodegenState

  runCodegen :: Codegen a -> a
  runCodegen = flip evalState initialState
    where
      initialState = CodegenState 0 M.empty M.empty
\end{minted}

Both finite maps are initially empty, and we start the label counter at zero.
We use a number of helper functions for modifying the state; in particular, we
have functions for generating a fresh, unique label from a given string, along
with two functions which take a function of type
\haskell{LocationMap -> LocationMap} and apply it to modify the
\haskell{offsets} and \haskell{heapLocs} map, respectively.

An important helper function that we use throughout the code generation is the
following:
\begin{minted}{haskell}
  concatMapM :: Monad m => (a -> m [b]) -> [a] -> m [b]
  concatMapM _ [] = pure []
  concatMapM f (x : xs) = do
    y <- f x
    ys <- concatMapM f xs
    pure $ y ++ ys
\end{minted}
%
The above is essentially a monadic version of
\haskell{concatMap :: (a -> [b]) -> [a] -> [b]} defined in the \haskell{GHC.List}
module, which takes some function \haskell{f :: a -> [b]} and maps it onto a
list of type \haskell{[a]}, subsequently \emph{flattening} the resulting
list of lists, concatenating all the lists within the outer list and returning
a single list of type \haskell{[b]}.

All our code generation functions have the return type \haskell{Codegen Program},
where we recall that \haskell{Program} is just a shorthand for a list of
instructions \haskell{[Instr]}.
A number of nodes in our AST feature a list---of declarations, statements,
or expressions---such as the arguments for a function call of type
\haskell{[Expr UType]}, or a statement block \haskell{[Stmt UType]}.
For these nodes, we wish to map the corresponding code generation function onto
the list, performing all stateful monad operations in sequence, and then
flatting the resulting list of lists into a single list of instructions.
This is precisely what \haskell{concatMapM} achieves, e.g. in the following
example:
%
\begin{minted}{haskell}
  codegenStmt :: Stmt UType -> Codegen Program
  ...
  codegenStmt (While _ cond loopStmts) = do
    condProgram <- codegenExpr cond
    loopProgram <- concatMapM codegenStmt loopStmts
    topLabel <- freshLabel "while"
    endLabel <- freshLabel "endwhile"
    pure $
      [Label topLabel] ++ condProgram ++ [BranchFalse endLabel]
      ++ loopProgram ++ [BranchAlways topLabel] ++ [Label endLabel]
\end{minted}

Here, \haskell{concatMapM} is used to generate instructions for
all the statements in the loop body, while simultaneously sequencing the monadic
effects performed while translating each of the statements to a list of
instructions. Thanks to the \haskell{do}-notation, we can simply assign the
monadic result to \haskell{loopProgram}, which we insert into the list of
instructions that we return in the last four lines.

The third and fourth line also show our \haskell{freshLabel} helper function in
action, which we call with the strings \haskell{"while"} and \haskell{"endwhile"}
to obtain unique labels we can use for the branch/jump instructions at the
top and bottom of the \spl{while} loop.



\section{Instructions}

We defined algebraic data types \haskell{Register} and \haskell{Instr} for the
register names and instructions available in SSM. An SSM program is simply a
list of instructions: \haskell{type Program = [Instr]}.
By defining \haskell{Show} instances for the registers and instructions, we can
easily print/output a \verb|.ssm| file from an instance of type
\haskell{[Instr]} with \haskell{unlines $ show <$> p}.

An excerpt of the data type for instructions and the corresponding \haskell{Show}
instance is given below.

\begin{minted}{haskell}
  data Instr = Label T.Text | Ret | Halt | Link Int | Unlink | Adjust Int | ...

  instance Show Instr where
    show (Label t) = T.unpack t <> ":"
    show Ret = tab <> "ret"
    show Halt = tab <> "halt"
    show (Link i) = tab <> "link" <+> show i
    show Unlink = tab <> "unlink"
    show (Adjust i) = tab <> "ajs" <+> show i
    ...
\end{minted}

We indent all instructions except for labels by two spaces, which helps with
navigating the resulting SSM code.
Currently, the code generation stage simply dumps the pretty-printed SSM
code into the console, to test the execution, we can simply pipe the console
output into a file, on which we then run the SSM interpreter with the
\texttt{--cli} option. We can achieve all the above with a simple bash
script:\todo{Is this worth discussing?}
%
\begin{minted}{bash}
  #!/bin/bash
  file=$1; # Take file path as command line arg
  stack build
  stack run -- codegen $file > output.ssm;
  java -jar ./../ssm/ssm.jar --file output.ssm --cli
\end{minted}


\section{Variables and Arguments}

As mentioned in \cref{sec:codegen-monad}, we track two maps in our
\haskell{CodeGen} monad state, which contain heap and stack locations,
respectively.
Since we allow local variable declarations only at the start of a function body,
we have essentially only one global scope with the global variable names,
and a local scope per function body, encompassing the parameter names of the
function along with any local variables declared at the start of the function
body.
When we encounter an identifier during code generation, we hence look for it
first in the \haskell{offsets} map for local variables and function arguments,
both of which are stored on the stack. If we cannot find the identifier there,
we check in the \haskell{heapLocs} map for a global variable entry, which are
the only identifiers stored directly on the heap.%
\footnote{Lists and tuples are always allocated on the heap, as we will discuss
in the following, though if they are an argument or local variable, the
corresponding identifier still refers to an offset on the stack, which contains
the heap location of the actual data.}
Identifier lookup is implemented by the following function:
%
\begin{minted}{haskell}
  lookupLoc :: T.Text -> Codegen Location
  lookupLoc ident = gets (M.lookup ident . offsets) >>= \case
    Nothing -> gets (M.lookup ident . heapLocs) >>= \case
      Nothing -> error $ "Couldn't find offset for identifer " <> T.unpack ident
      Just loc -> pure $ HeapLoc loc
    Just offset -> pure $ Offset offset
\end{minted}
%
Note that the error on the third line constitutes an internal compiler error,
rather than a user error: if the input program to the compiler contains unbound
identifiers, we should have already caught the issue in the typing stage and
generated an appropriate error message for the user.
As such, encountering an identifier for which we can't find an entry in our
location maps means that we have failed to discover a scoping error during the
typing stage, or that we have bugs in our code generation stage.



\section{Initialisation} \label{sec:codegen-initialisation}

Since the global variables can be accessed from anywhere in the program, we
choose to allocate them on the heap rather than the stack. Since we can compute
the size of the variable data statically, we can find them by adding an offset
to the heap location of the first global variable.
At the start of our compiled program, we include instructions for computing the
values of the global variables and storing them to the heap.

However, the global variables may themselves allocate data on the heap, e.g. if
we define a global variable as a list:
\begin{lstlisting}[language=spl]
  [Int] l = 1:2:3:[];
\end{lstlisting}
%
As a result, the location of the first global variable is not, in general, the
first address on the heap, and can in fact not be determined statically, since
the global variables may include arbitrary expressions which affect how many
heap registers are allocated.
Our solution is to first compute the values of \emph{all} global variables on
the stack, and subsequently storing the heap pointer to the first scratch
register \texttt{R5}, which we use simply for holding this `HeapLow' address.
We then store all global variables with a single \texttt{stmh} instruction,
ensuring that the data is allocated at the offsets from `HeapLow' that we
computed statically.

After the instruction for storing the global variables, we include a jump to the
\texttt{main} label, denoting the entry point of the actual program, followed by
the result of compiling all function declarations, and finally the \texttt{halt}
instruction:
%
\begin{minted}{haskell}
  codegen :: TA.Program UType UScheme -> Codegen Program
  codegen (TA.Program varDecls funDecls) = do
    let varSizes = map (\(VarDecl _ _ _ _ ty) -> uTypeSize ty) varDecls
    let varOffsets = computeOffsets varSizes 0
    varDeclsProgram <- concatMapM codegenGlobalVarDecl (zip varOffsets varDecls)
    funDeclsProgram <- concatMapM codegenFunMutDecl funDecls
    pure $ varDeclsProgram ++
      -- HP now at start of global vars, copy to R5
      [LoadReg HeapPointer, StoreReg HeapLowReg] ++
      -- Store all global vars to heap, adjust SP
      StoreHeapMulti (sum varSizes) : Adjust (-1) :
        BranchAlways "main" : funDeclsProgram ++ [Halt]
\end{minted}

% When compiling for a physical processor, it would likely be advantageous to
% allocate the global variables on the stack, rather than the heap, using a
% similar approach to the above. That is, we could again compute the values of all
% global variables on the stack, and then simply keep the address for the top (or
% bottom) of the block in one of the scratch registers, so that we can use it to
% compute the addresses of the global variables to load from and store to.


\section{Multi-Register Values} \label{sec:multi-reg-vals}

While values of the base types $\Int$, $\Bool$ and $\Char$ can all be
stored in a single register, value instances of lists
require---in general---multiple registers.
We also support tuples, which also require at least two registers:
values of type \code{\TProd{$\tau_1$}{$\tau_2$}} can be stored in two registers
when both $\tau_1$ and $\tau_2$ are base types.
The left and right component may again be multi-register values though, and
the programmer can nest tuples to an arbitrary depth, with types such as
$\TProd{\TProd{\Int}{\Bool}}{\TProd{\Int}{\Char}}$. In this case, we would
require four registers, within which we must compute the offsets when
translating e.g. the selectors \code{.fst} and \code{.snd}.

Perhaps more importantly, values of a list type \code{[$\tau$]} may require
arbitrarily many registers, even when $\tau$ is a base type such as $\Int$.
We store lists in a \emph{linked} representation on the heap, that is, as a
number of list segments which reference each other's location in memory.
More specifically, the $i$-th element of a list is stored in two components: the
value at the $i$-th position, along with the address at which the subsequent
element of the list is stored.
To terminate such a chain of references, we reserve the register value
\code{0xF0F0F0F0} as a `null pointer', similar to e.g. \code{null} in C or Java.
We use this null pointer in the final segment of a list to express that the
second component does not refer to the next segment, but instead terminates the
list. In this way, the null pointer is the SSM counterpart to SPL's `nil', or
\spl{[]}.

\subsection{List Representation Trade-Offs}
At this point, it may seem odd that we use such a linked representation of
lists, as it is rather inefficient in both space and time complexity when
compared to storing the list elements in an array of fixed size.
However, such a representation has some important advantages when dealing with
polymorphism, which we will cover in \cref{sec:polymorphism-boxing}, and in
mapping the semantics of SPL onto the instruction set of SSM.
The semantics of lists in SPL is much like that of lists in functional,
high-level languages such as ML/OCaml or Haskell, where lists are viewed as
essentially left-deep nested tuples, decomposable into a \emph{head} (the first
element of the list) and a \emph{tail}, the remainder of the list following the
head. This is reflected in the way SPL allows construction of lists, that is,
via the \emph{cons} constructor `\spl{:}' with type
$\forall \alpha.\ \alpha \to \TList{\alpha} \to \TList{\alpha}$, as well as the
destructors (selectors) on lists, namely \spl{.hd} and \spl{.tl}, which take a
non-nil list instance and return the head and tail, respectively.

Such a recursive representation of lists stands in contrast to the \emph{arrays}
that are a staple of imperative languages such as C or Java. These correspond
to a block of registers with fixed size, where the registers can be
accessed via indexing in $\bigO(1)$ time.
Importantly, the size of an array is fixed at instantiation and thus known
a-priori, while a recursive SPL list can grow and shrink at runtime. To express
such a semantics in SPL while storing the list elements in register blocks would
require an abstraction mechanism similar to e.g. Java's
\mintinline{java}|ArrayList<T>|, where the list is represented internally as an
array, along with the largest index currently in use. The internal array is then
upsized and downsized dynamically as necessary (or when reasonable).

Employing such a strategy in the code generation would result in significantly
faster access times for lists, since accesses are $\bigO(1)$ for array lists
too, compared to $\bigO(n)$ for linked lists.
Despite this, we chose the linked representation in SSM for the following reasons:
%
\begin{itemize}
  \item To benefit from the fast random access of arrays, SPL would need to be
        extended with a construct for index-based lookup in lists. With only the
        \spl{.hd} and \spl{.tl} selectors, the programmer could not leverage the
        improved access times in the first place.
  %
  \item Generating code for \code{ArrayList}-style lists would be a lot more
        complicated, especially in their interaction with polymorphism, and due
        to the fact that the size of lists in SPL cannot be determined
        statically. For instance, the length of a list may depend on user input,
        and thus our generated code would need to determine at runtime when an
        (array) list needs to be resized. At that point, it would make more
        sense to support an array (or rather, \emph{vector}) type in SPL
        directly, and then implement an \code{ArrayList}-style abstract
        interface within SPL, rather than in the compiler.
  %
  \item For lists with relatively few elements, the performance advantages of
        arrays are negligible, and even more so for lists which are frequently
        modified by appending or removing elements at the start of the list,
        which may even be slower with a dynamically resized array.
        These are common use cases in SPL, and so again, it would seem more
        reasonable to support a specific vector type for use cases where (very)
        large lists must be accessed often and out-of-order.
  %
  \item Using a representation of lists in SSM that is fundamentally different
        to the lists of SPL would lead to rather unpredictable performance from
        the user perspective. For example, extending a list
        $\code{l}$ of type $\TList{\Int}$ by \spl{l := 1:l;} could occasionally
        require copying all elements in \code{l}, causing the same operation to
        vary between being near-instant and very slow, which seems undesirable
        from the perspective of the programmer.
\end{itemize}

Based on the points above, we chose to translate SPL lists to a linked
representation in SSM, rather than a representation using blocks of registers.
Overall, we think it makes more sense to use an encoding that matches the lists
of SPL, and that for applications where constant-time list lookups are required,
the language could still be extended with a vector type distinct from the list
type $\TList{\tau}$, as well as syntax for index-based lookup in a vector, e.g.
with $\code{.(}i\code{)}$ as a new selector, where $i$ is an expression of type
$\Int$.


\subsection{Mutating Lists}
While SPL features functional-style lists (defined recursively as consisting of
a head element and remaining list), SPL also features mutable state, or more
specifically, mutable variables.
The combination of these two languages features allows mutating list instances
and the sublists of which they are comprised, which has some noteworthy and
potentially undesirable consequences.
For instance, consider the following program, which updates the tail of a list
using a variable assignment:
%
\begin{lstlisting}[language=spl]
  main() {
    [Int] l = 1:[];
    l.tl = l;
    $\ldots$
  }
\end{lstlisting}

Variable \spl{l} initially refers to the singleton list \spl{1:[]}, but then the
tail of \spl{l} is updated such that it refers to \spl{l} itself, causing the
list to become cyclically defined in terms of itself.
Creating a cyclical definition in this way is similar to the interaction of
anonymous functions and mutable references in a language such as OCaml, which
allow recursive fixed points to be defined by storing an anonymous function to a
mutable reference \code{r} and then updating the reference with a function that
uses \code{r} in its body, as in the following ML-style pseudocode:
\begin{lstlisting}
  let r := ref($\lambda$x.x) in
    r $\leftarrow$ $\lambda$x.((!r) x);
\end{lstlisting}

Returning to our list example: updating the tail of \spl{l} to refer to \spl{l}
itself effectively results in an infinite list, e.g. a recursive function which
prints all elements of a list would just repeatedly print \spl{1} when called
with \spl{l}.
Such behaviour could be desirable, as it allows us to effectively model
(infinite) streams. For instance, if we want to \emph{zip} a given list with the
list of alternating zeroes and ones using
\begin{center}
  \spl{zip :: [a] -> [b] -> [(a,b)]},
\end{center}
we could supply as the second argument cyclical list containing 0 and 1 in
alternation infinitely often, constructed similarly as above:
\begin{lstlisting}[language=spl]
  [Int] l = 0:1:[];
  l.tl = l;
\end{lstlisting}
%
Since the list of the zipped list is determined by the shorter of the two lists
given as inputs, the resulting list will be finite, even if one of the two
arguments is a self-referential, infinite list.




\section{Polymorphism and Boxing} \label{sec:polymorphism-boxing}

As pointed out in the previous section, our choice of representation for lists
and tuples in SSM is also informed by the code generation for polymorphic
functions, and their interaction with the data on which they operate.

To a simple example (perhaps the simplest), let us consider the polymorphic
identity function:
\begin{lstlisting}[language=spl]
  id(x) :: a -> a {
    return x;
  }
\end{lstlisting}
%
Throughout a given source program, this function may be called at a whole number
of different types, e.g. \spl{id(42)}, \spl{id(42:[])}, or \spl{id((40,2))}. For
translating such a function to SSM, we have two general approaches we can choose
between:
\begin{enumerate}[label={(\arabic*)}]
  \item \textbf{Monomorphisation.} We could generate a monomorphic instance of
        the \spl{id} function for each type at which the function is called. For
        the example calls above, we would then need instances for $\Int$,
        $\TList{\Int}$, as well as $\TProd{\Int}{\Int}$.
        Such an approach would allow us to use e.g. a multi-register
        representation of tuples on the stack. Since we generate a specific
        instance of \spl{id} for the tuple type $\TProd{\Int}{\Int}$, the
        monomorphic instance can account for the tuple argument spanning
        multiple registers, and handle it accordingly.
  %
  \item \textbf{Boxing.} Alternatively, we can choose to represent lists and
        tuples such that they can be treated uniformly with the single-register
        base types $\Int$, $\Bool$ and $\Char$.
        This approach involves \emph{boxing} all tuple and list values, that is,
        storing them entirely on the heap, and passing the location on the heap
        to the function as the argument.
        As a result, we can treat all types uniformly, and rather than
        generating a whole host of specialised monomorphic instances, we require
        only one instance of the \spl{id} function in SSM, which simply returns
        the value in the register for the first argument, without needing to
        consider the argument type in any way.
\end{enumerate}

One downside of (2) is that boxing tuples and lists results in a slight
performance decrease whenever we actually construct or destruct tuple and list
instances. In particular, instances of these types are always pointers to the
heap, so we must always first load from the heap when accessing the data.

A larger disadvantage of storing all tuples and lists on the heap is an increase
in memory usage. For instance, we might call a function which constructs and
returns a nested tuple value from which we select just one of the entries and
discard the rest.
In such a scenario, the entire tuple remains stored on the heap, and freeing the
occupied registers would require us to implement some form of memory management,
that is, moving around entries on the heap such that we can decrement the heap
pointer and free up registers whose data is no longer needed.

On the other hand, an issue with the monomorphisation approach of (1) is that it
may lead to a combinatorial explosion of specialised function instances.
Take, for example, the following function, which converts left-deep tuples into
right-deep tuples:
\begin{lstlisting}[language=spl]
  renest(t) :: ((a,b),c) -> (a,(b,c)) {
    var t1 = t.fst;
    return (t1.fst, (t1.snd, t.snd));
  }
\end{lstlisting}
%
The programmer may define such a polymorphic function and use it with numerous
unique combinations of types for the type variables \spl{a}, \spl{b} and
\spl{c}. If each type variable is instantiated with e.g. three different types,
we may already need up to 27 monomorphic function instances to cover all calls
to the \spl{renest} function.
We could of course mitigate this combinatorial increase somewhat by e.g.
treating at least all single-register base types uniformly, though even then
there would still be cases where we get many specialised instances, albeit for
slightly more contrived examples.


\section{Parameter Passing}
Beyond the considerations regarding the representation for values of various
data types covered so far, we must also pay attention to how our choices may
(inadvertently) affect the semantics resulting from our code generation.
In particular, the choice between the monomorphisation or boxing approach
covered in \cref{sec:polymorphism-boxing} naturally leads to two different
behaviours when passing tuples as function parameters, as we will illustrate
with the following example.

\subsection{Example: Tuple Parameter}
If we chose the monomorphisation approach for tuples and allocated all tuples on
the stack, then loading a tuple from e.g. a local or global variable would
result in a full copy, i.e. the entries (fields) are copied to
the top of the stack.

Let us consider the following function, which allocates a tuple in a local
variable, which it then passes to the function \spl{g}.
%
\begin{lstlisting}[language=spl]
  f(x) :: Int -> Int {
    (Int,Bool) t = (x,True);
    g(t);
    return t.fst;
  }

  g(t) :: (Int,a) -> Void {
    t.fst = t.fst + 1;
    return;
  }
\end{lstlisting}
%
In the first approach, loading a tuple results in both fields being placed on
the stack, such that \spl{g} is called with the \emph{data} from the tuple entries.
Modifications to the tuple performed in \spl{g} are thus not reflected in
\spl{f}. In the example above, \spl{g} increments the first entry of its
argument, but \spl{g} is operating on a copy of the tuple while in the scope of
\spl{f}, the variable \spl{t} refers to the original tuple data.
As such, the function call \spl{g(t)} in \spl{f} cannot affect the output of
\spl{f}. This behaviour is commonly referred to as \emph{pass-by-value}, since
the \emph{value} of \spl{t} (in this case, the $\Int$ and $\Bool$ field) is
passed to the function.

Pass-by-value stands in contrast to a \emph{pass-by-reference} semantics, where
\spl{g} instead receives a reference to \spl{t}, that is, the location of the
original data, rather than a copy.
We obtain such a semantics with the boxing approach, since the local variable
\spl{t} then stores the heap location of the tuple data. In the call \spl{g(t)},
the \emph{location} of \spl{t} is passed to \spl{g}, rather than the data itself,
and the incrementation performed in \spl{g} will thus also be reflected in the
scope of \spl{f}, since the original data on the heap is modified.
For example, the following \spl{main} function would print \spl{43} with
pass-by-reference, while for pass-by-value semantics it would print \spl{42}.
%
\begin{lstlisting}[language=spl]
  main() {
    print(f(42));
  }
\end{lstlisting}

While the two approaches described in \cref{sec:polymorphism-boxing} lend
themselves better to either a pass-by-value or pass-by-reference behaviour, it's
important to note that these two design choices are not directly related.
In principle, we could also combine polymorphism through boxing with
pass-by-value behaviour for tuples, or vice versa with monomorphisation and
pass-by-reference.

However, if we choose to store values of a certain type on the heap, there are
also practical reasons for using pass-by-reference, since it avoids copying all
the data at each function call.
This is especially relevant for lists, as a single list instance may contain
many elements, and thus require a large amount of heap memory.


\subsection{Lists}
Since we represent lists using linked segments on the heap, we think it is more
natural for lists to be (implicitly) passed as reference parameters, that is, a
heap location referring to the head segment of the list.
Of course, a consequence of this is that when passing a list to a function, the
function can modify the original list instance, as well as its sublists.








\begin{todoenv}
  \begin{itemize}
    \item Compilation scheme?
    \item How is data represented? Lists, tuples
    \item Semantics style, call-by-reference, call-by-value?
    \item How did you solve overloaded functions?
    \item Polymorphism?
    \item Printing?
    \item Problems?
    \item\ldots
  \end{itemize}
\end{todoenv}
