\chapter{Code Generation} \label{chp:codegen}

In the code generation stage, we use the type-annotated syntax tree from our
typing stage to generate code for the \emph{Simple Stack Machine}
(\emph{SSM})\footnote{\url{https://gitlab.science.ru.nl/compilerconstruction/ssm}}.
The type information we inferred and checked during the previous stage is used
during code generation, though only for a select few purposes.
In particular, we use the type annotations for resolving calls to the
\spl{print} function, as well as various sanity checks, where we throw an
internal error if we encounter invalid type information.
We do not need the type annotations to determine the size of data on the heap or
stack though, as we represent all data types uniformly in a single stack
register, where composite data types (i.e. lists and tuples) use a \emph{boxed}
representation, consisting of a pointer stored on the stack which refers to
the list/tuple data stored on the heap.

Of course, the semantic analyses serve a much more general purpose than just
supplying type annotations for the code generation stage, as they preclude many
possible errors that might arise during code generation or at runtime due to
mismatched types.

In the following, we briefly discuss our chosen evaluation strategy in
\cref{sec:codegen-eval-order}, after which we cover our general setup for
generating SSM code in \cref{sec:codegen-monad},
which concern the monad, data structures, and helper functions that we use, how
we represent the SSM instruction set in Haskell, and how we deal with scoping
and storing/loading for function arguments as well as local and global variables.
We briefly discuss the initialisation of global variables on the heap in
\cref{sec:codegen-initialisation}.

In \cref{sec:codegen-multi-reg-vals}, we weigh the benefits and drawbacks of possible
representations for multi-register (or composite) data, and motivate our corresponding
design choices. This directly leads us to the topic of polymorphism, which we
cover in \cref{sec:polymorphism-boxing}, which we conclude with a discussion on
the semantics of parameter passing.



\section{Evaluation Order} \label{sec:codegen-eval-order}

We take the evaluation strategy of SPL to be \emph{eager}\kern1pt\footnote{In the context
of e.g. the lambda calculus, eager evaluation is often referred to as \emph{call-by-value}, and
lazy evaluation as \emph{call-by-name}, though call-by-value is also used to
specify that parameters are passed in value representation, rather than as references. To avoid
confusion, we use pass-by-value and pass-by-reference for the latter distinction.}
meaning that function arguments are evaluated prior to performing function
application. For instance, in the function call \spl{f(1+1)}, we first evaluate
the expression \spl{1+1} to \spl{2} before performing any computation in the
\emph{body} (that is, the definition) of the function \spl{f}.
In more practical terms, this means that the generated SSM instructions will
first reduce all argument expressions, such that these are in value form on the
stack before we jump to the respective function.

As a consequence, we may perform unnecessary evaluation of arguments if they are
never used in the function body, though the upside is that values only require
a fixed amount of space (for the base types \spl{Int}, \spl{Bool}, and
\spl{Char} at least---we will talk about lists in detail in
\cref{sec:codegen-multi-reg-vals}), while a \emph{lazy} evaluation strategy
would require us to store unevaluated expressions, which may be arbitrarily
large. The memory usage for lazy evaluation is thus significantly smaller, and
we avoid complicated memory management involving the storing and loading of
arbitrary expressions.



\section{Code Generation Infrastructure} \label{sec:codegen-monad}

Our code generation stage makes use of the state monad, where the state consists
of a label counter of type \haskell{Int}, along with two maps from
\haskell{Text} to \haskell{Int}.
The label counter simply allows us to obtain `fresh', previously unused integers
by taking the current value and then incrementing the counter. We use the label
counter in the labels we generate for \spl{if} and \spl{while} statements, for
which we use labels `else', `endif', and so on, and then append a fresh integer
to the name to ensure that the label name is unique, and doesn't clash with the
labels generated for other parts of the code.

Of the two finite maps, the first one is for storing offsets on the stack for
local variables and function arguments, and the other for heap locations, i.e.
global variables.

\begin{minted}{haskell}
  type LocationMap = M.Map T.Text Int

  data CodegenState = CodegenState {
    labelCounter :: Int,
    offsets :: LocationMap,
    heapLocs :: LocationMap
  }

  type Codegen = State CodegenState

  runCodegen :: Codegen a -> a
  runCodegen = flip evalState initialState
    where
      initialState = CodegenState 0 M.empty M.empty
\end{minted}

Both finite maps are initially empty, and we start the label counter at zero.
We use a number of helper functions for modifying the state; in particular, we
have functions for generating a fresh, unique label from a given string, along
with two functions which take a function of type
\haskell{LocationMap -> LocationMap} and apply it to modify the
\haskell{offsets} and \haskell{heapLocs} map, respectively.

An important helper function that we use throughout the code generation is the
following:
\begin{minted}{haskell}
  concatMapM :: Monad m => (a -> m [b]) -> [a] -> m [b]
  concatMapM _ [] = pure []
  concatMapM f (x : xs) = do
    y <- f x
    ys <- concatMapM f xs
    pure $ y ++ ys
\end{minted}
%
The above is essentially a monadic version of
\haskell{concatMap :: (a -> [b]) -> [a] -> [b]} defined in the \haskell{GHC.List}
module, which takes some function \haskell{f :: a -> [b]} and maps it onto a
list of type \haskell{[a]}, subsequently \emph{flattening} the resulting
list of lists, that is, concatenating all the lists within the outer list and
returning a single list of type \haskell{[b]}.

All our code generation functions have the return type \haskell{Codegen Program},
where we recall that \haskell{Program} is just a shorthand for a list of
instructions \haskell{[Instr]}.
A number of nodes in our AST feature a list---of declarations, statements,
or expressions---such as the arguments for a function call of type
\haskell{[Expr UType]}, or a statement block \haskell{[Stmt UType]}.
For these nodes, we wish to map the corresponding code generation function onto
the list, performing all stateful monad operations in sequence, and then
flatting the resulting list of lists into a single list of instructions.
This is precisely what \haskell{concatMapM} achieves, e.g. in the following
example:
%
\begin{minted}{haskell}
  codegenStmt :: Stmt UType -> Codegen Program
  ...
  codegenStmt (While _ cond loopStmts) = do
    condProgram <- codegenExpr cond
    loopProgram <- concatMapM codegenStmt loopStmts
    topLabel <- freshLabel "while"
    endLabel <- freshLabel "endwhile"
    pure $
      [Label topLabel] ++ condProgram ++ [BranchFalse endLabel]
      ++ loopProgram ++ [BranchAlways topLabel] ++ [Label endLabel]
\end{minted}

Here, \code{concatMapM} is used to generate instructions for
all the statements in the loop body, while simultaneously sequencing the monadic
effects performed while translating each of the statements to a list of
instructions. Thanks to the \haskell{do}-notation, we can simply assign the
monadic result to \code{loopProgram}, which we insert into the list of
instructions that we return in the last four lines.

The third and fourth line also show our \code{freshLabel} helper function in
action, which we call with the strings \haskell{"while"} and \haskell{"endwhile"}
to obtain unique labels we can use for the branch/jump instructions at the
top and bottom of the \spl{while} loop.



\subsection{Instructions} \label{sec:codegen-instructions}

We defined algebraic data types \haskell{Register} and \haskell{Instr} for the
register names and instructions available in SSM. An SSM program is simply a
list of instructions: \haskell{type Program = [Instr]}.
By defining \haskell{Show} instances for the registers and instructions, we can
easily print/output a \verb|.ssm| file from an instance of type
\haskell{[Instr]} with \code{unlines \$ show <\$> p}.

An excerpt of the data type for instructions and the corresponding \haskell{Show}
instance is given below.

\begin{minted}{haskell}
  data Instr = Label T.Text | Ret | Halt | Link Int | Unlink | Adjust Int | ...

  instance Show Instr where
    show (Label t) = T.unpack t <> ":"
    show Ret = tab <> "ret"
    show Halt = tab <> "halt"
    show (Link i) = tab <> "link" <+> show i
    show Unlink = tab <> "unlink"
    show (Adjust i) = tab <> "ajs" <+> show i
    ...
\end{minted}

We indent all instructions except for labels by two spaces, which makes the
resulting SSM code slightly easier to navigate.
% The code generation stage simply dumps the pretty-printed SSM code into the
% console.
% To run the SSM code, we can then pipe the console output into a
% file, on which we then run the SSM interpreter.
% We can achieve the above with a simple bash script:\todo{Is this worth
% discussing?}
% %
% \begin{minted}{bash}
%   #!/bin/bash
%   file=$1; # Take file path as command line arg
%   stack build
%   stack run -- codegen $file > output.ssm;
%   java -jar ./../ssm/ssm.jar --file output.ssm --cli
% \end{minted}


\subsection{Variables and Arguments} \label{sec:codegen-vars-args}

As mentioned in \cref{sec:codegen-monad}, we track two maps in our
\haskell{CodeGen} monad state, which contain heap and stack locations,
respectively.
Since we allow local variable declarations only at the start of a function body,
we have essentially only one global scope with the global variable names,
and a local scope per function body, encompassing the parameter names of the
function along with any local variables declared at the start of the function
body.
When we encounter an identifier during code generation, we hence look for it
first in the \haskell{offsets} map for local variables and function arguments,
both of which are stored on the stack. If we cannot find the identifier there,
we check in the \haskell{heapLocs} map for a global variable entry, which are
the only identifiers stored directly on the heap.%
\footnote{Lists and tuples are always allocated on the heap, as we will discuss
in the following, though if they are an argument or local variable, the
corresponding identifier still refers to an offset on the stack, which contains
the heap location of the actual data.}
Identifier lookup is implemented by the following function:
%
\begin{minted}{haskell}
  lookupLoc :: T.Text -> Codegen Location
  lookupLoc ident = gets (M.lookup ident . offsets) >>= \case
    Nothing -> gets (M.lookup ident . heapLocs) >>= \case
      Nothing -> error $ "Couldn't find offset for identifer " <> T.unpack ident
      Just loc -> pure $ HeapLoc loc
    Just offset -> pure $ Offset offset
\end{minted}
%
Note that the error on the third line constitutes an internal compiler error,
rather than a user error: if the input program to the compiler contains unbound
identifiers, we should have already caught the issue in the typing stage and
generated an appropriate error message for the user.
As such, encountering an identifier for which we can't find an entry in our
location maps means that we have failed to discover a scoping error during the
typing stage, or that we have bugs in our code generation stage.



\section{Program Initialisation} \label{sec:codegen-initialisation}

Since the global variables can be accessed from anywhere in the program, we
choose to allocate them on the heap rather than the stack. Since we can compute
the size of the variable data statically, we can find them by adding an offset
to the heap location of the first global variable.
At the start of our compiled program, we include instructions for computing the
values of the global variables and storing them to the heap.
However, doing so may allocate data on the heap, e.g. if we generate a list:
\begin{lstlisting}[language=spl]
  [Int] l = 1:2:3:[];
\end{lstlisting}
%
% As a result, the location of the first global variable is not, in general, the
% first address on the heap, and can in fact not be determined statically, since
% the global variables may include arbitrary expressions which affect how many
% heap registers are allocated.
Our solution is to first compute the values of \emph{all} global variables on
the stack, which may result in heap register allocation. We subsequently store
the heap pointer to the first scratch register \texttt{R5}, which we use simply
for holding this `HeapLow' address. We then store all global variables with a
single \texttt{stmh} instruction, ensuring that the data is allocated at the
offsets from `HeapLow' that we computed.

After the instructions for storing the global variables, we include a jump to
the \texttt{main} label, denoting the entry point of the actual program,
followed by the result of compiling all function declarations, and finally the
\texttt{halt} instruction:
%
\begin{minted}{haskell}
  codegen :: TA.Program UType UScheme -> Codegen Program
  codegen (TA.Program varDecls funDecls) = do
    let varSizes = map (\(VarDecl _ _ _ _ ty) -> uTypeSize ty) varDecls
    let varOffsets = computeOffsets varSizes 0
    varDeclsProgram <- concatMapM codegenGlobalVarDecl (zip varOffsets varDecls)
    funDeclsProgram <- concatMapM codegenFunMutDecl funDecls
    pure $ varDeclsProgram ++
      -- HP now at start of global vars, copy to R5
      [LoadReg HeapPointer, StoreReg HeapLowReg] ++
      -- Store all global vars to heap, adjust SP
      StoreHeapMulti (sum varSizes) : Adjust (-1) :
        BranchAlways "main" : funDeclsProgram ++ [Halt]
\end{minted}

% When compiling for a physical processor, it would likely be advantageous to
% allocate the global variables on the stack, rather than the heap, using a
% similar approach to the above. That is, we could again compute the values of all
% global variables on the stack, and then simply keep the address for the top (or
% bottom) of the block in one of the scratch registers, so that we can use it to
% compute the addresses of the global variables to load from and store to.


\section{Composite Data Types} \label{sec:codegen-multi-reg-vals}

While values of the base types $\Int$, $\Bool$ and $\Char$ can all be
stored in a single stack entry, value instances of lists
require---in general---multiple entries, or addresses.
We also support tuples, which also require at least two entries:
values of type \code{\TProd{$\tau_1$}{$\tau_2$}} can be stored in two entries
when both $\tau_1$ and $\tau_2$ are base types.
The left and right component may again be composite values though, and
the programmer can nest tuples to an arbitrary depth, e.g. the type
$\TProd{\TProd{\Int}{\Bool}}{\TProd{\Int}{\Char}}$ would require four stack
entries.

Perhaps more importantly, values of a list type \code{[$\tau$]} may require
arbitrarily many memory cells, even when $\tau$ is a base type such as $\Int$.
We store lists in a \emph{linked} representation on the heap, that is, as a
number of list segments which reference each other's location in memory.
More specifically, the $i$-th element of a list is stored in two components: the
value at the $i$-th position, along with the address at which the subsequent
element of the list is stored.
To terminate such a chain of references, we reserve the register value
\code{0xF0F0F0F0} as a `null pointer', similar to e.g. \code{null} in C or Java.
We use this null pointer in the final segment of a list to express that the
second component does not refer to the next segment, but instead terminates the
list. In this way, the null pointer is the SSM counterpart to SPL's \spl{[]}.

Consider, for example, the following variable declaration:
\begin{lstlisting}[language=SPL]
  var l = 1:2:3:[];
\end{lstlisting}
%
Here, the value stored for \spl{l} is an address, pointing to the first
segment of the list on the heap. The allocation of the list results in the heap
layout shown in \cref{fig:list-heap-layout}.

\begin{figure}[b]
  \centering
  \begin{tabular}{|>{\ttfamily}l| >{\ttfamily}l| l|}
    \hline
    \textbf{\textrm{Address}} & \textbf{\textrm{Value}} & \textbf{Field} \\
    \hline
    0x0007D0 & 0x000003 & \spl{.hd} \\
    0x0007D1 & 0xF0F0F0 & \spl{.tl} \\
    \hline
    0X0007D2 & 0X000002 & \spl{.hd} \\
    0X0007D3 & 0X0007D1 & \spl{.tl} \\
    \hline
    0X0007D4 & 0X000001 & \spl{.hd} \\
    0X0007D5 & 0X0007D3 & \spl{.tl} \\
    \hline
  \end{tabular}
  %
  \caption{Heap layout (excerpt) after allocating the list \spl{1:2:3:[]}}
  \label{fig:list-heap-layout}
\end{figure}



\subsection{List Representation Trade-Offs}
At this point, it may seem odd that we use such a linked representation of
lists, as it is rather inefficient in both space and time complexity when
compared to storing the list elements in an array of fixed size.
However, such a representation has some important advantages when dealing with
polymorphism, which we will cover in \cref{sec:polymorphism-boxing}, and in
mapping the semantics of SPL onto the instruction set of SSM.
The semantics of lists in SPL follows that of functional, high-level languages
such as ML/OCaml or Haskell, where lists are viewed as essentially left-deep
nested tuples, decomposable into a \emph{head} (the first element of the list)
and a \emph{tail}, the remainder of the list following the head. This is
reflected in the way SPL allows construction of lists, that is, via the
\emph{cons} constructor `\spl{:}' with type
$\forall \alpha.\ \alpha\ \TList{\alpha} \to \TList{\alpha}$, as well as the
destructors (selectors) on lists, namely \spl{.hd} and \spl{.tl}, which take a
non-nil list instance and return the head and tail, respectively.

Such a recursive representation of lists stands in contrast to the \emph{arrays}
that are a staple of imperative languages such as C or Java. These correspond
to a block of memory cells with fixed size, where the entries can be
accessed by their index in $\bigO(1)$ time.
Importantly, the size of an array is fixed at instantiation and thus known
a-priori, while a recursive SPL list can grow and shrink at runtime. To express
such a semantics in SPL while storing the list elements in register blocks would
require an abstraction mechanism similar to e.g. Java's
\mintinline{java}|ArrayList<T>|, where the list is represented internally as an
array, along with the largest index currently in use. The internal array is then
upsized and downsized dynamically as necessary (or when reasonable).

Employing such a strategy in the code generation would result in significantly
faster access times for lists, since accesses are $\bigO(1)$ for array lists
too, compared to $\bigO(n)$ for linked lists.
Despite this, we chose the linked representation in SSM for the following reasons:
%
\begin{itemize}
  \item To benefit from the fast random access of arrays, SPL would need to be
        extended with a construct for index-based lookup in lists. With only the
        \spl{.hd} and \spl{.tl} selectors, the programmer could not leverage the
        improved access times in the first place.
  %
  \item Generating code for \code{ArrayList}-style lists would be a lot more
        complicated, especially in their interaction with polymorphism, and due
        to the fact that the size of lists in SPL cannot be determined
        statically. For instance, the length of a list may depend on user input,
        and thus our generated code would need to determine at runtime when an
        (array) list needs to be resized. At that point, it would make more
        sense to support an array (or rather, \emph{vector}) type in SPL
        directly, and then implement an \code{ArrayList}-style abstract
        interface within SPL, rather than in the compiler.
  %
  \item For lists with relatively few elements, the performance advantages of
        arrays are negligible, and even more so for lists which are frequently
        modified by appending or removing elements at the start of the list,
        which may even be slower with a dynamically resized array.
        These are common use cases in SPL, and so again, it would seem more
        reasonable to support a specific vector type for use cases where (very)
        large lists need to be accessed often and out-of-order.
  %
  \item Using a representation of lists in SSM that is fundamentally different
        to the lists of SPL would lead to rather unpredictable performance from
        the user perspective. For example, extending a list
        $\code{l}$ of type $\TList{\Int}$ by \spl{l := 1:l;} could occasionally
        require copying all elements in \code{l}, causing the same operation to
        vary between being near-instant and very slow. Such behaviour seems
        undesirable from the perspective of the programmer.
\end{itemize}

Based on the points above, we chose to translate SPL lists to a linked
representation in SSM, rather than a representation using blocks of registers.
Overall, we think it makes more sense to use an encoding that matches the lists
of SPL, and that for applications where constant-time list lookups are required,
the language could still be extended with a vector type distinct from the list
type $\TList{\tau}$, as well as syntax for index-based lookup in a vector, e.g.
with $\code{.(}i\code{)}$ as a new selector, where $i$ is an expression of type
$\Int$.


\subsection{Mutating Lists}
While SPL features functional-style lists (defined recursively as consisting of
a head element and remaining list), SPL also features mutable state, or more
specifically, mutable variables.
The combination of these two languages features allows mutating list instances
and the sublists of which they are comprised, which has some noteworthy and
potentially undesirable consequences.
For instance, consider the following program, which updates the tail of a list
using a variable assignment:
%
\begin{lstlisting}[language=spl]
  main() {
    [Int] l = 1:[];
    l.tl = l;
    $\ldots$
  }
\end{lstlisting}

Variable \spl{l} initially refers to the singleton list \spl{1:[]}, but then the
tail of \spl{l} is updated such that it refers to \spl{l} itself, causing the
list to become cyclically defined in terms of itself.
% Creating a cyclical definition in this way is similar to the interaction of
% anonymous functions and mutable references in a language such as OCaml, which
% allow recursive fixed points to be defined by storing an anonymous function to a
% mutable reference \code{r} and then updating the reference with a function that
% uses \code{r} in its body, as in the following ML-style pseudocode:
% \begin{lstlisting}
%   let r := ref($\lambda$x.x) in
%     r $\leftarrow$ $\lambda$x.((!r) x);
% \end{lstlisting}

Updating the tail of \spl{l} to refer to \spl{l} itself effectively results in
an infinite list, e.g. a recursive function which prints all elements of a list
would just repeatedly print \spl{1} when called with \spl{l} (until the stack
overflows, at least).
Such behaviour could be desirable, as it lets us model infinite, periodic lists.
For instance, if we want to \emph{zip} a given list with the list of alternating
zeroes and ones using
\begin{center}
  \spl{zip :: [a] -> [b] -> [(a,b)]},
\end{center}
we could supply as the second argument the cyclical list containing 0 and 1 in
alternation infinitely often, constructed similarly as above:
\begin{lstlisting}[language=spl]
  [Int] l = 0:1:[];
  l.tl = l;
\end{lstlisting}
%
Since the list of the zipped list is determined by the shorter of the two lists
given as inputs, the resulting list will be finite, even if one of the two
arguments is a self-referential, infinite list.




\section{Polymorphism and Parameter Passing} \label{sec:polymorphism-boxing}

As pointed out in the previous section, our choice of representation for lists
and tuples in SSM is also informed by the code generation for polymorphic
functions.
To a simple example (perhaps the simplest), let us consider the polymorphic
identity function:
\begin{lstlisting}[language=spl]
  id(x) :: a -> a {
    return x;
  }
\end{lstlisting}
%
Throughout a given source program, this function may be called at a whole number
of different types, e.g. \spl{id(42)}, \spl{id(42:[])}, or \spl{id((40,2))}. For
translating such a function to SSM, we have two general approaches we can pick
between:
\begin{enumerate}[label={(\arabic*)}]
  \item \textbf{Monomorphisation.} We could generate a monomorphic instance of
        the \spl{id} function for each type at which the function is called. For
        the example calls above, we would then need instances for $\Int$,
        $\TList{\Int}$, as well as $\TProd{\Int}{\Int}$.
        Such an approach would allow us to use e.g. a multi-register
        representation of tuples on the stack. Since we generate a specific
        instance of \spl{id} for the tuple type $\TProd{\Int}{\Int}$, the
        monomorphic instance can account for the tuple argument spanning
        multiple registers, and handle it accordingly.
  %
  \item \textbf{Boxing.} Alternatively, we can choose to represent lists and
        tuples such that they can be treated uniformly with the single-register
        base types $\Int$, $\Bool$ and $\Char$.
        This approach involves \emph{boxing} all tuple and list values, that is,
        storing them entirely on the heap, and passing the location on the heap
        to the function as the argument.
        As a result, we can treat all types uniformly, and rather than
        generating a whole host of specialised monomorphic instances, we require
        only one instance of the \spl{id} function in SSM, which simply returns
        the value in the register for the first argument, without needing to
        consider the argument type in any way.
\end{enumerate}

Of these two, we choose approach (2), that is, we use a \emph{pass-by-value}
semantics only for the base types, and use \emph{pass-by-reference} for all
composite types.

\paragraph{Further considerations}
One downside is that this results in a slight performance decrease whenever we
construct, access or update instances of tuples and lists. In particular,
instances of these types are always pointers to the heap, so we must first load
from the heap when accessing the data.
Another disadvantage is an increase in memory usage. For example, if we call
a function which internally constructs a tuple, the tuple instance will remain
stored on the heap, even after we lose access to the pointer.
Freeing the occupied registers would require us to implement some form of memory
management, that is, moving around entries on the heap such that we can
decrement the heap pointer and free up registers whose data is no longer needed.

On the other hand, an issue with the monomorphisation approach of (1) is that it
may lead to a combinatorial explosion of specialised function instances.
% Take, for example, the following function, which converts left-deep tuples into
% right-deep tuples:
% \begin{lstlisting}[language=spl]
%   renest(t) :: ((a,b),c) -> (a,(b,c)) {
%     var t1 = t.fst;
%     return (t1.fst, (t1.snd, t.snd));
%   }
% \end{lstlisting}
%
A programmer may a function with multiple polymorphic arguments and use it with
numerous combinations of types, which could lead to a large amount of distinct
combinations for which we must generate monomorphic instances.
We could mitigate this combinatorial increase somewhat by e.g. treating at least
all single-register base types uniformly, though even then there would still be
cases where we get many specialised instances, albeit for slightly more
contrived examples.

\paragraph{Parameter passing semantics}
Beyond the considerations regarding the representation for values of various
data types covered so far, we must also pay attention to how our choices may
(inadvertently) affect the semantics resulting from our code generation.
In particular, the boxing and monomorphisation approach for dealing with
polymorphism naturally lead to two different behaviours when passing tuples as
function parameters, as we will illustrate with the following example.

\paragraph{Tuple parameter example}
If we keep all tuple data on the stack and use pass-by-value, then calling a
function with a tuple would pass that function a full copy of the data, i.e. all
entries are copied to the top of the stack before branching.

Let us consider the following function, which stores a tuple in a local
variable, which it then passes to the function \spl{g}.
%
\begin{lstlisting}[language=spl]
  f(x) :: Int -> Int {
    (Int,Bool) t = (x,True);
    g(t);
    return t.fst;
  }

  g(t) :: (Int,a) -> Void {
    t.fst = t.fst + 1;
    return;
  }
\end{lstlisting}
%
Using pass-by-value, \spl{g} is called with the \emph{data} from the tuple
entries. Modifications to the tuple performed in \spl{g} are thus not reflected
in \spl{f}. In the example above, \spl{g} increments the first entry of its
argument, but \spl{g} is operating on a copy of the tuple while in the scope of
\spl{f}, the variable \spl{t} refers to the original tuple data. As such, the
function call \spl{g(t)} in \spl{f} cannot affect the output of \spl{f}.

By choosing the `boxing' approach, we instead obtain a pass-by-reference
semantics for tuples, where \spl{g} instead receives a reference to \spl{t},
that is, the location of the original data, rather than a copy.
The local variable \spl{t} then stores the heap location of the tuple data. In
the call \spl{g(t)}, the \emph{location} of \spl{t} is passed to \spl{g}, rather
than the data itself, and the incrementation performed in \spl{g} will thus also
be reflected in the scope of \spl{f}, since the original data on the heap is
modified. For example, the following \spl{main} function would print \spl{43}
with pass-by-reference, while for pass-by-value semantics it would print
\spl{42}.
%
\begin{lstlisting}[language=spl]
  main() {
    print(f(42));
  }
\end{lstlisting}

While the boxing approach described in \cref{sec:polymorphism-boxing} lends
itself better to a pass-by-reference semantics, and monomorphisation lends
itself better to pass-by-value, it is important to note that these two design
choices are not directly related.
In principle, we could also combine polymorphism through boxing with
pass-by-value behaviour for tuples, or vice versa with monomorphisation and
pass-by-reference.

However, if we choose to store values of a certain type on the heap, there are
also practical reasons for using pass-by-reference, since it avoids copying all
the data at each function call.
This is especially relevant for lists, as a single list instance may contain
many elements, and thus require a large amount of heap memory.


\paragraph{List parameter passing}
Since we represent lists using linked segments on the heap, we think it is more
natural for lists to be (implicitly) passed as reference parameters, that is, a
heap location referring to the head segment of the list.
Of course, a consequence of this is that when passing a list to a function, the
function can modify the original list instance, as well as its sublists.



\section{Overloaded Functions}

When generating code for the overloaded comparison operators and the \spl{print}
function, we use the type annotations generated by the typing stage, in
particular the annotations which are added by the elaboration step, described
in \cref{sec:elaboration}.

The types allow us to disambiguate between calls to the overloaded functions
at different types, so that we can pick the appropriate implementation.
For the comparison operators, we actually just generate the corresponding
instruction, e.g. \code{eq} or \code{ne}.
Since all types is represented in a single entry on the stack, this yields
correct comparison of integers and characters, and gives a way of checking
object equality, or reference equality, for composite data types.

The \spl{print} function is only correctly implemented for the base types, where
we use the \code{trap} instruction for printing integers and characters.
For Booleans, we include a printing subroutine, and generate an instruction for
branching to the subroutine for an application of \spl{print} with a Boolean
argument.
For composite data types, we just print the value as an integer, that is, the
heap address where the data is stored. This might not be the behaviour that the
programmer expects, but we felt that it might be useful for debugging purposes.

In order to support object equivalence for \code{==} and full printing of
composite data types with \spl{print}, we would need to use a form of
\emph{dictionary passing.}
Implementing overloading via dictionary passing would also require support for
passing functions as parameters: if we want to print an instance of e.g. type
\spl{[$\tau$]}, we would need to call the print instance for lists, which in turn
would expect as its input the printing function for its elements of type $\tau$.

While allowing functions as arguments/inputs would be fairly straightforward,
supporting functions as outputs (i.e. partial function application, or currying)
would be more challenging, and we thus consider full printing and object
equivalence for composite types to be out of scope.



% \begin{todoenv}
%   \begin{itemize}
%     \item Compilation scheme?
%     \item How is data represented? Lists, tuples
%     \item Semantics style, call-by-reference, call-by-value?
%     \item How did you solve overloaded functions?
%     \item Polymorphism?
%     \item Printing?
%     \item Problems?
%     \item\ldots
%   \end{itemize}
% \end{todoenv}
