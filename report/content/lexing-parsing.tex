\chapter{Lexing \& Parsing} \label{chp:lexing-parsing}

In this chapter we discuss the lexing and parsing phases of our compiler.
For both phases we make use of the Megaparsec
library\footnote{\url{https://hackage.haskell.org/package/megaparsec}}.
%
Our compiler has two separate phases for lexing and parsing.
This is mainly to make the work of the parser easier through the preprocessing
performed by the lexer.
The two phases also constitute a separation of concerns: the lexer
tokenises the input, converting e.g. literals and keywords into corresponding
tokens, and handling whitespace and comments; the parser deals with the actual
program structure and the construction of the AST from the token stream returned
by the lexer.
%
As a consequence, we require an additional pass for lexing the input prior to
parsing, and the lexer cannot leverage the program structure to e.g. exclude
certain cases or provide more informative error messages.
However, as we will see, the parser benefits in many ways from this
preprocessing.

We first cover the lexing stage in \cref{sec:lexing}, after which we discuss
the setup of parsing stage in \cref{sec:parsing-setup}, our parse AST
representation in \cref{sec:parse-ast}, and the parsing of programs in
\cref{sec:parsing-programs}.


\section{Lexing Stage} \label{sec:lexing}

The lexer has two main functionalities.
Firstly, it deals with stripping whitespace and comments, which includes
skipping single-line and multi-line comments.
Secondly, this stage tokenises the text input, converting the source file into a
stream of tokens.

Our lexer is implemented using Megaparsec, and has the following type:
\begin{minted}{haskell}
  type Lexer a = Parsec Void Text a
\end{minted}
%
The type of the input stream is \haskell{Text}. We use \haskell{Void} for the
custom error type, as Megaparsec's built-in error type is sufficient for
handling the few forms of error we might encounter during lexing.
%
Our lexer instances are generally of type \haskell{Lexer Token}, that is, they
try to consume part of the input, returning a token if they succeed.

Tokens are one of either \vspace{-1mm}
\begin{itemize}[itemsep=0mm]
  \item integer, Boolean, or character literals;
  \item keywords and symbols, as defined in our keyword and symbol table; or
  \item tokens for function/variable identifiers.
\end{itemize}
%
The above is captured by the following data type:
\begin{minted}{haskell}
  data Token = IntLit Int | BoolLit Bool | CharLit Char
             | IdToken T.Text
             | NameToken T.Text
             | Symbol Symbol
             | Keyword Keyword
    deriving (Eq, Ord)
\end{minted}


\paragraph{Literals}
The lexer tokenises integer, Boolean and character literals, converting them to
instances of \haskell{IntLit Int}, \haskell{BoolLit Bool} or \haskell{CharLit Char},
respectively.
For integers, we attempt to read in a sequence of decimal characters,
for Booleans we attempt to read one of the strings \haskell{"True"} or
\haskell{"False"}, and for character literals we attempt to read an alphanumeric
character, space or punctuation symbol between two single quotation marks
\haskell{"\'"}.

In each case, we require that the sequence is not followed by further
alphanumeric characters or underscores (or just letters, in the case of
integers), so e.g. the Boolean lexer would succeed if the next character is
non-alphanumeric, e.g. \spl{True;} or \spl{True&}, but would fail on \spl{True1}
or \spl{True_}, thereby allowing identifiers (for which lexing is attempted
later) to have \spl{True} or \spl{False} as a prefix.


\paragraph{Keywords and symbols}
All keywords and symbols are enumerated in the data types \haskell{Keyword} and
\haskell{Symbol}, where \haskell{Keyword} includes the statement keywords
\spl{if}, \spl{else}, \spl{while}, \spl{return}, the variable declaration
keyword \spl{var}, the type names \spl{Int}, \spl{Bool}, \spl{Char}, \spl{Void},
and the field selectors \spl{hd}, \spl{tl}, \spl{fst}, and \spl{snd}.

The symbols cover all the unary and binary operators, including those consisting
of two characters, such as \spl{\|\|}, \spl{&&}, \spl{=}\spl{=}, \spl{!=}, and
other reserved symbols such as \spl{::} and \spl{->} for type signatures, or
\spl{=}, \spl{;}, \spl{\{} and \spl{\}} for statements and code blocks.
%
\begin{minted}{haskell}
  data Keyword = KwVar | KwIf | KwElse | ... | KwFst | KwSnd

  instance Show Keyword where
    show KwVar     = "var"
    show KwIf      = "if"
    show KwElse    = "else"
    ...
    show KwFst     = "fst"
    show KwSnd     = "snd"

  data Symbol = SymColonColon | SymRightArrow | ... | SymBraceLeft | SymBraceRight

  instance Show Symbol where
  show SymColonColon    = "::"
  show SymRightArrow    = "->"
  ...
  show SymBraceLeft     = "{"
  show SymBraceRight    = "}"
\end{minted}

We can then use the Enum library to obtain a list of all keywords/symbols
instances in the data type, with which we can then define the corresponding
lexer, e.g. in the case of symbols:
%
\begin{minted}{haskell}
  symbols :: [Symbol]
  symbols = enumFromTo minBound maxBound

  symbolL :: Lexer Token
  symbolL = choice $ (\sym -> string (T.pack (show sym)) >> pure (Symbol sym)) <$> symbols
\end{minted}
%
The \code{choice} function will try the alternatives in the given list of lexers
in order, returning immediately if one of them succeeds, and failing otherwise.
For each symbol \code{sym}, the list we supply to \code{choice} includes a lexer
that attempts to lex the output of \code{show sym}, and returns the token
\haskell{Symbol sym} if it succeeds.

Defining the symbol and keyword lexer in this way allows us to modify the
symbols and keywords by simply updating the data type declarations and show
instances, without needing to touch any code in the actual lexer.
A slight caveat is that we need to pay attention to the order in which we
declare the constructors of \haskell{Symbol} when
symbols have overlapping prefixes, as is the case for e.g.
\spl{:} and \spl{::}, or \spl{[} and \spl{[]} (the empty list).\footnote{The
same is true for keywords, but our SPL implementation does not feature any
keywords with overlapping prefixes.}
We then need to ensure that we attempt to lex the longer symbol before its
prefix, which we achieve by declaring it earlier in the data type definition.


\paragraph{Identifiers}
The last alternative in the lexer is that for identifiers, which can be any
letter character\footnote{We actually reserve capitalised names for data
constructors, further details on which we defer to \cref{chp:extension}.}
followed by a sequence of alphanumeric characters or underscores.
Identifiers cover global and local variable names, function names, and
function parameter names.

\begin{minted}{haskell}
  identL :: Lexer Token
  identL = do
    name <- T.cons <$> letterChar <*> (T.pack <$> many (alphaNumChar <|> char '_'))
    pure $ IdToken name
\end{minted}


\paragraph{Source locations}
During tokenisation, the lexer also annotates all tokens with their position in
the file. The lexer returns a list of \mintinline{haskell}{Positioned Token},
where \mintinline{haskell}{Positioned a} is defined as follows:
%
\begin{minted}{haskell}
  data Positioned a = Positioned {
    startPos :: SourcePos,
    endPos :: SourcePos,
    startOffset :: Int,
    tokenLength :: Int,
    tokenVal :: a
  }

  data SourcePos = SourcePos {
    sourceName :: FilePath,
    sourceLine :: !Pos,
    sourceColumn :: !Pos
  }
\end{minted}

These source locations are propagated through the stages of our compiler,
allowing us to specify source code locations in error messages throughout the
compilation process, which remain accurate even after translating our AST
representation in the desugaring phase (\cref{sec:desugaring}).
%
All user errors result in error messages featuring a file name together with a
line and column range, indicating the part of the program from which the error
arose. If supported by the shell/terminal (e.g. that of VSCode), these links are
even clickable, allowing the user to immediately jump to the location of the
error in their source code.

\paragraph{Whitespace and comments}
Our parsing library of choice, Megaparsec, makes dealing with whitespace quite
straightforward. The library allows one to define a \emph{space consumer}, which
can consume spaces, tabs, line breaks, line comments and block comments
according to delimiters specified by the client.
By defining the delimiters \code{//} as well as \code{/*} and \code{*/}, we can
instruct Megaparsec to strip all line and block comments from the input.
There is even a built-in option that we can use to ensure correct handling of
nested block comments.

\paragraph{Error recovery}
The lexer supports error recovery, which is quite simple to achieve.
There are essentially just two types of errors that can occur while lexing:
either we encounter an invalid integer (e.g. \verb|123ab|),
or we encounter a symbol not featured in the symbol table.
In both cases we recover from the error by repeatedly discarding characters from
the input until we can successfully lex a token again.
In any case, we fail after the lexing stage and do not continue to parsing, but
the lexer error recovery allows us to identify multiple errors in one lexing
pass and generate error messages for each of them.



\section{Parse AST Representation} \label{sec:parse-ast}

Before we can explain how a token stream returned by the lexer is parsed, we
first need to introduce the representation of programs that our parser targets,
that is, our representation of abstract syntax trees.
As briefly mentioned in \cref{sec:intro-lang-choice}, the syntax of SPL
corresponds to a form of \emph{rose trees}, where certain AST nodes have a
sequence of successors, or \emph{sub-forest}, rather than a fixed number of
child nodes.
For instance, we have an arbitrary number of top-level variable declarations at
the start of a program, and similarly for e.g. the statements in a function body
or while-block.

When working with Haskell, the obvious choice is to use (recursive) algebraic
data types, which we combine with the built-in list type to represent the
aforementioned nodes with a sequence of successors.
The data types of our AST representation can be found in \cref{fig:parse-ast}.

The structure generally follows that of the grammar given in \cref{chp:grammar},
that is, we have data types corresponding to variable declarations and function
declarations, statements, expressions, types, along with some additional data
types for field lookups as well as unary and binary operators.
We first remark on some general features of the AST representation, before
briefly discussing the data declarations.


\begin{figure}
\begin{minted}[breaklines]{haskell}
data Program = Program [VarDecl] [FunDecl]
data VarDecl = VarDecl Loc (Maybe Type) T.Text Expr
data FunDecl = FunDecl Loc T.Text [T.Text] (Maybe Type) [VarDecl] [Stmt]

data Stmt = If Loc Expr [Stmt] [Stmt]
          | While Loc Expr [Stmt]
          | Assign Loc VarLookup Expr
          | FunCall Loc T.Text [Expr]
          | Return Loc (Maybe Expr)
          | GarbageStmt

data Type = IntT Loc | BoolT Loc | CharT Loc
          | Prod Loc Type Type | List Loc Type | Void Loc
          | Fun Loc [Type] Type
          | TyVar Loc T.Text
          | GarbageType

data VarLookup = VarId Loc T.Text | VarField Loc VarLookup Field
data ExprLookup = ExprField Expr Field
data Field = Head | Tail | Fst | Snd | GarbageField

data Expr = Ident Loc T.Text
          | ExprLookup Loc ExprLookup
          | Int Loc Int
          | Bool Loc Bool
          | Char Loc Char
          | UnOp Loc UnaryOp Expr
          | BinOp Loc BinaryOp Expr Expr
          | FunCallE Loc T.Text [Expr]
          | EmptyList Loc
          | Tuple Loc Expr Expr
          | GarbageExpr

data UnaryOp = Not | Neg
data BinaryOp = Add | Sub | Mul | Div | Mod | Eq | Neq | Lt | Gt | Lte | Gte
              | And | Or | Cons
\end{minted}
	\caption{Abstract syntax tree for parsing stage}
  \label{fig:parse-ast}
\end{figure}


\paragraph{Location data}
For most of the AST nodes, we include a field with location data of type
\haskell{Loc}, which is defined as follows:
%
\begin{minted}{haskell}
  data Loc = Loc !SourcePos !SourcePos
\end{minted}
%
The two entries of type \haskell{SourcePos} correspond to a start and end
position, which our parser determines based on the token positions provided by
the lexer.
Such a \haskell{Loc} entry thus specifies the part of the input which a given
AST node corresponds to. The location data is propagated throughout the
remaining stages of the compiler, allowing us to specify a range within the
source code when reporting errors to the user, such as a type mismatch
discovered in the typing stage.


\paragraph{`Garbage' nodes}
At each level of the AST, we include a `garbage' AST node,
such as \haskell{GarbageStmt} for statements, or \haskell{GarbageExpr} for
expressions. In the case of a parse error, these nodes allow us to construct a
valid AST and continue our parsing pass, while also clearly marking the AST as
erroneous. When we later translate our parse AST to the AST representation used
for typing, we include sanity checks to ensure that no garbage nodes are
present; otherwise we would have entered the typing stage despite the presence
of syntax errors, which would clearly constitute a compiler bug.
Since the `garbage' nodes allow us to continue parsing after we encounter a
parse error, they play an important role for supporting multiple errors
and error recovery.


\paragraph{Program and declarations}
Our top level node is \haskell{Program}. A \haskell{Program} is given by a list
of variable declarations, and a list of function declarations.
The two forms of declaration are represented by \haskell{VarDecl} and
\haskell{FunDecl}, where we again see the use of lists in \haskell{FunDecl}
represent a sequence of parameter names and the sequence of variable
declarations and statements.
Both forms of declaration feature an optional type annotation, represented using
the \haskell{Maybe} type constructor.


\paragraph{Statements}
The cases for statements exactly match those in the grammar, except for the
additional \haskell{GarbageStmt} case. Much like a function body, the statement
blocks of if-else and while-loops are represented by a list of statements,
the arguments in a function call are a list of expressions, and we use
\haskell{Maybe} for the optional expression in a return statement.

Variable assignments can feature field selectors on the left-hand side, e.g.
\spl{x.fst = 42;}. We model this using \haskell{VarLookup}, an instance of which
is either an identifier or again an instance of \haskell{VarLookup}, so that we
can represent chaining of selectors as in e.g. \spl{x.tl.hd.fst = 42;}.
The chain of selectors is then represented by a nesting of \haskell{VarField}s,
terminated by \haskell{VarId}.


\paragraph{Types}
We use \haskell{Type} to represent types annotated by the user, where type
variables are given by arbitrary identifiers, e.g. \spl{a [a] -> [a]}, or
\spl{(left,right) -> (right,left)}. Type variables are given by the case
\haskell{TyVar Loc T.Text}, that is, we store the identifiers as specified by
the user.


\paragraph{Expressions}
The identifier case for expressions is again of represented using
\haskell{Text}. For field lookups/projections, we use \haskell{ExprLookup}
and the \haskell{Field} data type, the cases of which correspond to the list and
tuple projections \spl{hd}, \spl{tl}, \spl{fst} and \spl{snd}.
By the mutual recursion between \haskell{Expr} and \haskell{ExprLookup}, field
lookups can be chained, much like in variable assignments, as in e.g.
\spl{((1,2):[]).hd.fst}.

All unary and binary operations are represented by the cases \haskell{UnOp} and
\haskell{BinOp}, where the operators are given by \haskell{UnaryOp} and
\haskell{BinaryOp}, respectively.
Function calls are captured by \haskell{FunCallE}, with the function identifier
of type \haskell{Text} and the arguments given by a list of expressions
\haskell{[Expr]}.
We did not create a special representation for built-in functions like
\spl{print} or \spl{isEmpty}. We essentially treat these functions just like
regular user-defined functions, except that their type and implementation are
predetermined, and are hard-coded in the typing and code generation stage.

% \paragraph{Field lookups}
% To model field lookups we use the \haskell{Field} datatype, featuring cases for
% the list and tuple projections \code{hd}, \code{tl}, \code{fst} and \code{snd}.
% Field selectors can appear on the left side of assignments, e.g.
% \spl{x.fst = 42;}, and in expressions, e.g. \spl{(0,40+2).snd}.
% In both variable assignments and expressions, field selectors can be chained
% arbitrarily, e.g. \spl{x.tl.tl.hd.fst}.
% For field lookups in expressions, we have the \haskell{ExprLookup} case,
% featuring a selector of type \haskell{Field} and a

% For expressions, we have the type \haskell{ExprLookup}, with a single
% constructor featuring an expression and a field selector of type \haskell{Field}.

% Similarly, \haskell{ExprLookup} is mutually recursive with \haskell{Expr}, so in
% a field lookup of the form $e.s$, $e$ can be an arbitrary expression, so can
% again be of the form $e.s$, and so on.




\section{Parsing Stage} \label{sec:parsing}

Before we discuss details on the parsing of programs, we give a more technical
account of the underlying `machinery' of the parsing stage in
\cref{sec:parsing-setup}. In particular, we cover the custom token stream type
on which our parser operates, how we define custom parse errors, and our use of
monadic parser combinators and helper functions.
\cref{sec:parser-error-handling} covers our support for error recovery and
reporting multiple errors, and \cref{sec:precedence-associativity} explains how
we handle operator precedence and associativity when parsing expressions.


\subsection{Parsing Setup} \label{sec:parsing-setup}

While \haskell{Lexer} operates on input of type \haskell{Text}, our parser
\haskell{TokenParser} operates on the token stream returned by the lexer.
The type of our parser is given by:
%
\begin{minted}{haskell}
  type TokenParser a = Parsec ParserError TokenStream a
\end{minted}
Here, \haskell{TokenStream} specifies the type of the input stream,
\haskell{ParserError} is the type of our custom error component, and the type
variable \code{a} (made explicit above by eta-expansion) specifies the output
type of the parser instance.


\paragraph{Custom token stream}

Our lexer instances generally return a single token, that is, an instance of
the \haskell{Token} data type shown in \cref{sec:lexing}. By lexing many such
tokens from the input, we obtain a list of tokens, hence the input of type
\haskell{Text} is processed into a list \haskell{[Positioned Token]},
where \haskell{Positioned a} equips the underlying type \code{a} with positional
information:
\begin{minted}{haskell}
  data Positioned a = Positioned {
    startPosition :: SourcePos,
    endPosition   :: SourcePos,
    startOffset   :: Int,
    tokenLength   :: Int,
    tokenVal      :: a
  } deriving (Eq, Ord, Show)
\end{minted}

In order to use Megaparsec with our custom token type, we need to make our list
of tokens an instance of Megaparsec's \haskell{Stream} type class\footnote{\url{https://hackage.haskell.org/package/megaparsec-9.7.0/docs/Text-Megaparsec-Stream.html}}.
Simply instantiating \haskell{Stream} with \haskell{[Positioned Token]} is not
sufficient though, since we want our parser to display segments or lines from
the original input in our error messages, which is not possible based on the
token stream alone.

To this end, we must bundle the list of tokens returned by the lexer with the
original source program of type \haskell{Text}:
%
\begin{minted}{haskell}
  data TokenStream = TokenStream {
    tokenStreamInput :: T.Text,
    tokenStream :: [Positioned PT.Token]
  } deriving (Show)
\end{minted}
%
We give type class declarations of \haskell{TokenStream} for the \haskell{Stream}
type class, along with the \haskell{VisualStream} and \haskell{TraversableStream}
type classes, which are needed for error reporting and debugging support.
This involves a fair amount of function definitions, though most of the
implementation closely follows the example code given in the Megaparsec tutorial
of Mark Karpov~\citep{Karpov2019}, which discusses the use of Megaparsec with custom input
streams\footnote{\url{https://markkarpov.com/tutorial/megaparsec.html\#working-with-custom-input-streams}}.

Since this is really just boilerplate code, we will forego a detailed
explanation, but give a brief summary.
We need to define a number of functions for decomposing a \haskell{TokenStream}
instance into a (list of) \haskell{Positioned Token} and the remaining stream,
along with functions for displaying a \haskell{TokenStream} instance as a
string, and a function that takes an offset in the input and returns the
corresponding line, while also updating the positional state of the stream,
shifting it to the position given by the offset.


\paragraph{Custom parse errors}

Megaparsec can generate so-called trivial parse errors out of the box, where the
error message displays the position of the parse error and the corresponding
line from the input, listing the unexpected token that was encountered (if any),
and the tokens that were expected.
We can additionally generate `fancy' custom parse errors. A naive method for
doing so is using the \haskell{fail} combinator, which always fails, reporting
its string argument as the error.

A cleaner approach is to use Megaparsec's support for extending its
\haskell{ErrorFancy} type by embedding a custom data type of parse errors.
We use this capability to define various custom errors for different scenarios,
with a few example cases shown below:
%
\begin{minted}{haskell}
  data ParserError where
    NoClosingDelimiter   :: PT.Symbol -> ParserError
    FunctionNoArrow      :: ParserError
    ProdTypeMissingEntry :: ParserError
    ...
    IfNoCondition        :: ParserError
\end{minted}

We can then specify how our custom errors should be displayed by defining a
corresponding type class instance for Megaparsec's \haskell{ShowErrorComponent}:
%
\begin{minted}{haskell}
  instance ShowErrorComponent ParserError where
    showErrorComponent :: ParserError -> String
    showErrorComponent (NoClosingDelimiter sym) = "Expecting closing `" <> show sym <> "`"
    showErrorComponent FunctionNoArrow = "Expecting argument type or `->`"
    showErrorComponent ProdTypeMissingEntry = "Expecting left type of product"
    ...
    showErrorComponent IfNoCondition = "Expecting if condition between `(` and `)`"
\end{minted}

Instances of \haskell{ParserError} are used throughout our parser implementation
to provide some more context in error messages, rather than merely listing
expected and unexpected tokens.
Using a data type to do so allows us to reuse our custom errors in different
parts of the parser, and ensures that the various forms of error are always
displayed in a consistent format.


\paragraph{Parser combinators}
Our parser consists of monadic parsing functions for our \haskell{TokenParser}
type constructor, defined in terms of Megaparsec's \haskell{Parsec} monad.
As is typical for a purely functional parser using monadic functions, we make
heavy use of monadic \emph{combinators}, such as \code{many} and
\code{sepBy} from \haskell{Control.Monad}.
The parser monad is in fact an instance of \haskell{MonadPlus}, meaning we also
have access to the functions of the \haskell{Alternative} type class, most
notably \code{<|>}, which denotes a choice between a number of parser
functions, or \code{optional}, which allows a parser to fail, returning a
result wrapped in \haskell{Maybe}.

We also define various helper functions to abstract out reoccurring patterns,
e.g. as parsing a symbol token or parsing some part of the program between
delimiters, as illustrated below:
\begin{minted}{haskell}
  symbolP :: Symbol -> TokenParser (WithPos ())
  symbolP s = token test S.empty
    where
      test t | tokenVal t == Symbol s = Just ((), t.startPosition, t.endPosition)
      test _ = Nothing

  betweenP :: TokenParser (WithPos open) -> TokenParser (WithPos close) ->
              TokenParser a -> TokenParser (WithPos a)
  betweenP openP closeP p = do
    (_,start,_) <- openP
    x <- p
    (_,_,end) <- closeP
    pure (x,start,end)
\end{minted}
%
The function \code{symbolP} takes an argument \code{s} of type
\haskell{Symbol} (as discussed in \cref{sec:lexing}) and returns a parser that
tests whether the next token in the token stream is a symbol token for the
symbol \code{s}.
The \haskell{WithPos} type constructor simply equips the result type with the
start and end position in a 3-tuple:
\begin{minted}{haskell}
  type WithPos a = (a, SourcePos, SourcePos)
\end{minted}
Most of our parser helpers include positions in their output; we process these
in the higher-level parsers to determine the \haskell{Loc} entries in the AST.

The second function, \code{betweenP}, accepts two \haskell{TokenParser}
instances that parse an opening and closing delimiter, and a third parser
\code{p} that is run between the delimiters.
We can then parse an expression of the form \code{($e$)} as follows, where $e$
is parsed by the argument of type \haskell{TokenParser a}:
\begin{minted}{haskell}
  parensP :: TokenParser a -> TokenParser (WithPos a)
  parensP = betweenP (symbolP SymParenLeft) (symbolP SymParenRight)
\end{minted}

Generally speaking, our parser functions form a hierarchy, starting with
predicates for the various token types, then helper functions such as
\code{parensP}, and then parser functions for the various components and
variants of statements, expressions, and so on, (as discussed in
\cref{sec:parsing-programs}), which are combined to parsers for variable
declarations and function declarations at the top level.


\subsection{Error Handling} \label{sec:parser-error-handling}

Our parsing stage supports reporting of multiple parse errors, and can recover
from many forms of errors, in particular when a syntactic component is missing
altogether, or by using the delimiters of statements to skip ahead and continue
parsing in an attempt to find and report further errors.


\paragraph{Reporting multiple errors}

By identifying and reporting multiple syntax errors, we can provide the user
with more information in the presence of multiple syntax errors; doing so allows
them to locate and fix more errors without re-running the compiler.

By default, when our top-level instance of \haskell{TokenParser} fails, the
parsing pass is terminated and the error is reported (with a location and the
offending line from the input).
Aside from letting a \haskell{TokenParser} instance fail outright, Megaparsec
allows us to report an error without failure, such that we can continue parsing
and potentially find and report further errors.
We use this to handle certain `predictable' errors, especially those where some
symbol or subexpression is missing outright. We can then simply register the
error, continue parsing, and report the error later.

Here is where the `garbage' nodes mentioned in \cref{sec:parse-ast} come
into play: if e.g. the condition of an if-statement is missing, we can use a
\haskell{GarbageExpr} node to still construct a valid AST, allowing the
statement parser to return a result such that we can continue parsing
afterwards:
\begin{minted}{haskell}
  parensP exprP <|> registerError (withDummyPos GarbageExpr) IfNoCondition
\end{minted}
%
The parser above will attempt to parse an expression between parentheses, using
the \code{parensP} function we saw previously.
If parsing the expression fails, we register a custom \haskell{IfNoCondition}
error and return a \haskell{GarbageExpr} (with dummy position data) for the
\haskell{Expr} component of the \haskell{If} node.


To give another example, consider the case of the empty list literal
\spl{[]} as part of our expression parser:
\begin{minted}{haskell}
  emptyListP = do
      (_,start,_) <- symbolP SymBracketLeft
      (_,_,end) <- symbolP SymBracketRight <|>
          registerError (withDummyPos ()) (NoClosingDelimiter SymBracketRight)
\end{minted}
%
In the above code, we first try to parse a left bracket `\code{[}'. If
\code{symbolP} \haskell{SymBracketLeft} succeeds, we assume that the user intended to
type `\code{[]}', since that is the only possible expression starting with a
left bracket.
If parsing the right bracket token fails, we register a corresponding
error (with \haskell{NoClosingDelimiter} and the symbol `\code{]}'),
but do not fail outright, allowing \code{emptyListP} to succeed despite the
missing bracket.


Error correction is of course of a heuristic nature, as we are essentially
trying to guess the intent of the user and handle the syntax error accordingly.
In the above example, the user may also have entered an opening bracket
`\code{[}' by accident; if we then report further errors in the following input,
the user might be more confused than if we reported only the missing closing
bracket `\code{]}'.


\paragraph{Error recovery}

For other situations, e.g. upon encountering an unexpected token, we can instead
discard tokens from the input in an attempt to recover the parsing pass.
We achieve this by wrapping a parser in a recovery combinator, where if the
inner parser fails, another parser function is called in an attempt to recover
from the failure and resume parsing at some later point in the input stream.
Importantly, if an error was registered during parsing, our compiler will always
terminate at the end of the parsing stage, and will never proceed to the
following stages with some corrected or modified AST.

SPL statements are terminated with `\code{;}', and we require that
statement blocks are always surrounded with braces, i.e. `\verb|{...}|'.
These delimiters---when placed correctly---provide reference points that we can
use for error recovery when parsing sequences/blocks of statements.
When parsing the statements in a function body, if-block, else-block, or
while-block, we make use of Megaparsec's \code{withRecovery} function with the
following recovery parser:
\begin{minted}{haskell}
  parseToNextStmt :: ParseError TokenStream ParserError -> TokenParser Stmt
  parseToNextStmt e = (symbolP SymBraceRight >> fail "") <|>
    (skipManyTill (satisfy $ const True)
        (symbolP SymSemicolon <|> symbolP SymBraceRight) >>
      registerParseError e >> withRecovery parseToNextStmt stmtP)
\end{minted}

The above function is called when the statement parser \code{stmtP} fails.
In an attempt to find the start of the next statement, we skip tokens in the
input until we reach either a `\code{;}' or `\verb|}|'.
We then register the original error and call the statement parser at the
following token, again with error recovery via the \code{parseToNextStmt}
function itself.

Crucially, the first line of \code{parseToNextStmt} ensures that once we reach
the end of the current statement block, i.e. the closing brace `\verb|}|', the
recovery attempt fails outright.
Consider e.g. the following code snippet, where lines 2 and 3 in the
while-block feature a syntax error:
%
\begin{lstlisting}[language=spl,numbers=left]
  while (t < 100) {
    b = + b;
    t = t+1);
    l = b:l;
  }
  return l;
\end{lstlisting}
%
Given the above input (as a token stream returned by the lexer), our parser will
report an unexpected `\code{+}' in line 2. The recovery function
\code{parseToNextStmt} is called with the first error, and skips to the next
semicolon, after which parsing is resumed---again with error recovery---for the
statement in line 3. Again, an error is reported, this time the unexpected
closing parenthesis `\code{)}'. Recovery is again run with
\code{parseToNextStmt}, now skipping to line 4, where the variable assignment is
parsed successfully.

The above example illustrates a few key points regarding our error recovery:
\begin{enumerate}[label=(\arabic*)]
  \item To recover parsing, we must encounter a valid statement before the end
        of the current statement block. That is, if the statement on line 4 was
        also invalid, the error recovery would reach the closing brace at the
        end of the block and fail outright.
  %
  \item We can also recover on certain invalid statements, provided that all
        errors are caught by the parser. For instance, we could recover on the
        line \spl{l = [;}, since the missing closing bracket would be caught by
        the \code{emptyListP} parser discussed previously.
        By contrast, we cannot recover on the statement \spl{l = ];}, as our
        expression parser does not catch the unexpected closing bracket.
  %
  \item Recovery should not escape the current statement block. If we fail to
        resume parsing inside the while-block, we could in principle skip past
        the end of the while-block and successfully recover by parsing the
        statement on line 6.
        However, this has the unwanted effect of discarding the closing brace,
        and would likely lead to confusing errors in the outer parser, i.e. the
        parser for the surrounding block or function body.
        As such, we are better off failing outright at the closing brace with
        one useful error, rather than reporting multiple errors at the cost of
        confusing the user with errors that no longer match the source program.
\end{enumerate}

Again, error recovery is very much heuristic, and we cannot account for every
scenario. If, for instance, there are delimiters `\code{;}' or `\verb|}|'
missing in the input, our error recovery may lead to worse error messages, as
outlined in (3) above.


\subsection{Rewriting the Expression Grammar} \label{sec:precedence-associativity}

In order to write parser functions corresponding to the provided grammar of SPL,
we need to transform the grammar such that we eliminate occurrences of left
recursion and obtain the desired structure of the AST with respect to
associativity and operator precedence.
This pertains primarily to the parsing of expressions, where the rule
\[ \NT{Exp} \Coloneqq \NT{Exp}\ \NT{Op2}\ \NT{Exp} \]
is left-recursive and must be rewritten.

\paragraph{Operator precedence}
In addition, it is desirable from the user perspective to assign precedence
levels for the various binary operators supported by SPL, and treat them as
either left-associative or right-associative, depending on the operator.
We must take this into consideration when designing the rules of our parser
grammar, which we use as the basis for defining our parser combinators.

To illustrate, let us consider what AST our parser should generate for the
expression
\[ \code{2 == 1+1 \&\& 2 != 3-2} .\]
One would expect this to denote a conjunction of two (in)equalities, rather than
(for instance) an equality between \code{2} and everything right of the
`\code{==}', or an inequality between `\code{2 == 1+1 \&\& 2}' and `\code{3-2}'.
%
The latter two options of course make no sense, but more importantly, they do
not match the operator precedence typically assumed in written formulas or other
programming languages, so the programmer would not expect the expression to be
parsed as such.

Ideally, our parser should yield the desired AST without requiring the
programmer to add parentheses, hence the precedence of \code{\&\&} must be lower
than that of \code{==} and \code{!=}.
Similarly, all comparison operators should have a lower precedence than the
\emph{cons} operator `\code{:}', which in turn should be lower than addition
and subtraction, followed by multiplication, division and modulo and finally
Boolean and integer negation.

\paragraph{Expression grammar levels}
We obtain the desired operator precedences by using a recursive descent parser
with multiple levels based on the grammar shown in \cref{fig:new-expr-grammar},
where each non-terminal corresponds to a level in the precedence hierarchy of
operators.
%
The parser thus begins at the lowest precedence level, featuring the Boolean
operators \code{\&\&} and \code{||}.
In the rule for $\NT{Expr}$, the leftmost non-terminal is $\NT{Prop}$ and not
$\NT{Expr}$ itself, which prevents infinite parsing loops caused by left
recursion.
%
Below the expression level, denoted by $\NT{Expr}$, our grammar
features the following levels, in increasing precedence level (where some level
names are rather arbitrary):
\begin{itemize}
  \item Propositions $\NT{Prop}$, which include the comparison operators;
  \item $\NT{List}$, with the \spl{:} (cons) operator on lists;
  \item formulas $\NT{Form}$, featuring addition and subtraction;
  \item terms $\NT{Term}$, featuring multiplication, division, and the modulo
        operator;
  \item unary operations $\NT{UnOp}$, with the \spl{!} and \spl{-} operator;
  \item field selections $\NT{Sel}$, consisting of an atom followed by a
        (possibly empty) list of field selectors;
  \item and finally `atoms' $\NT{Atom}$, which include integer, Boolean and
        character literals, the empty list \spl{[]}, function calls, identifiers, tuples, or just a pair of
        parentheses, which modify the precedence.
\end{itemize}

\begin{figure}[th]
  \[
  \begin{array}{lcl}
		%
		\NT{Expr} & \Coloneqq & \NT{Prop}\ ((\code{\&\&} \mid \code{||})\ \NT{Prop})^* \br
		%
		\NT{Prop} & \Coloneqq & \NT{List}\ ((\code{==} \mid \code{!=} \mid \code{<} \mid \code{>} \mid \code{<=} \mid \code{>=})\ \NT{List})^* \br
		%
		\NT{List} & \Coloneqq & \NT{Form}\ (\code{:}\ \NT{Form})^* \br
		%
		\NT{Form} & \Coloneqq & \NT{Term}\ (( \code{+} \mid \code{-} )\ \NT{Term})^* \br
		%
		\NT{Term} & \Coloneqq & \NT{UnOp}\ (( \code{*} \mid \code{/} \mid \code{\%} )\ \NT{UnOp} )^* \br
    %
    \NT{UnOp} & \Coloneqq & \code{!}\ \NT{UnOp}\ \ruleSep \code{-}\ \NT{UnOp} \ruleSep \NT{Sel} \br
    %
    \NT{Sel} & \Coloneqq & \NT{Atom}\ (\code{.}\ \NT{Field})^* \br
    %
    \NT{Atom} & \Coloneqq & \NT{Int} \ruleSep \NT{Bool} \ruleSep \NT{Char} \\
    & \mid & \NT{Id} \ruleSep \NT{FunCall} \ruleSep \code{[]} \\
    & \mid & \code{(}\ \NT{Expr}\ \code{)} \ruleSep
        \code{(}\ \NT{Expr}\ \code{,}\ \NT{Expr}\ \code{)} \\
		%
		% \NT{Val} & \Coloneqq & \code{(}\ \NT{Expr}\ \code{)} \ruleSep
    %   \code{(}\ \NT{Expr}\ \code{,}\ \NT{Expr}\ \code{)} \\
		% & \mid & \NT{FunCall} \ruleSep
    %   \NT{Id} \NT{Field} \\
    % & \mid & \NT{Int} \ruleSep \NT{Bool} \ruleSep \NT{Char} \\
		% & \mid & \code{!}\ \NT{Val} \ruleSep \code{-}\ \NT{Val} \ruleSep \code{[]}
		%
  \end{array}
  \]
  \caption{Parser grammar for expressions}
  \label{fig:new-expr-grammar}
\end{figure}
%

The rule for $\NT{Atom}$ allows us to jump back to $\NT{Expr}$ at the topmost
level of the grammar, either via an opening parenthesis \spl{(}, or the
arguments of a function call \code{$f$($e_1$,$\dots$,$e_n$)}.

In order to still parse arbitrarily many applications of \code{\&\&} or
\code{||}, we use the pattern $((\code{\&\&} \mid \code{||})\ \NT{Prop})^*$,
hence we can parse any expression of the form $\dots \code{\&\&} \dots \code{||} \dots$,
or just a single $\NT{Prop}$, since $*$ denotes zero or more occurrences of
a pattern; the same format of production rule is also used for the subsequent
levels of the grammar.

The rules for $\NT{Prop}$, $\NT{List}$, $\NT{Form}$ and $\NT{Term}$ all follow
the same pattern; they consist of a non-terminal of the next level,
followed by zero or more occurrences of some binary operators along with another
non-terminal of the next level. The resulting expression grammar after applying
the transformations outlined above is shown in \cref{fig:new-expr-grammar}.


\paragraph{Associativity}

Another important consideration for the expression grammar is the
\emph{associativity} of the various operators. For instance, since addition
associates to the left, we want the expression \code{1+2+3} to result in an AST
of the form \code{Add (Add 1 2) 3} rather than \code{Add 1 (Add 2 3)} (see
\cref{fig:parse-ast} for the actual AST data type of our implementation).
In fact, all binary operators of SPL are usually taken to be left-associative,
in contrast to e.g. exponentiation, which is typically right-associative.

We obtain the corresponding left-deep AST through the general grammar rule
pattern $\NT{e}\ (\NT{op}\ \NT{e})^*$ for binary operations discussed previously.
Here, we first parse an instance of $\NT{e}$, which we then optionally combine
repeatedly with another instance of $\NT{e}$. We can do so using an
accumulator $a$, which we initialise with the leftmost subexpression
$e_1$, then update by $a \leftarrow \mathit{op}\ a\ e_2$, then by
$a \leftarrow \mathit{op}\ a\ e_3$, and so forth. By successively combining the (nested)
expression parsed thus far with the following subexpression, we achieve the
desired left-deep nesting of the AST.

Certain terminals and non-terminals, such as $\NT{Char}$ or \code{[]}, could be
placed higher up in the grammar, since e.g. characters and the empty list cannot
be used in arithmetic operations. Despite this, we chose to place these in the
$\NT{Val}$ rule, since it is more intuitive to provide a type error rather than
a parse error for non-numbers occurring in an arithmetic expression.


\subsection{Parsing of Programs} \label{sec:parsing-programs}

Our monadic parser functions closely follow the grammar given in
\cref{chp:codegen}, with the expression parser using the modified grammar as
detailed in \cref{sec:precedence-associativity}. We give some examples for the
parser functions at the top-level, the statement level, and the expression
level, showing how the concepts covered in the previous sections are used
concretely in the implementation.

Our top-level parser is defined as follows:
\begin{minted}{haskell}
  programP :: TokenParser Program
  programP = do
    varDecls <- many varDeclP
    funDecls <- many funDeclP
    handleNoFunDecls funDecls
    Program dataDecls varDecls funDecls <$ eof
\end{minted}
%
We first parse a (possibly empty) sequence of variable declarations using
\code{varDeclP}, followed by a sequence of function declarations with
\code{funDeclP}.
Global variable declarations begin with either the \spl{var} keyword token or
a monomorphic type, so there is no overlap with function declarations, which
must begin with an identifier token.
We do not directly enforce the presence of at least one function declaration
with the \code{some} combinator; instead, we use \code{many}, but register an
error if the resulting list of \haskell{FunDecl}s is empty, which lets us inform
the user that a program must define at least a \spl{main()} function.

A good example for our basic form of error recovery is the parser for variable
declarations:
\begin{minted}{haskell}
  varDeclP :: TokenParser VarDecl
  varDeclP = do
    (mty,startPos) <- varOrTypeP
    (ident,_,_) <- idP <|> registerError (withDummyPos (T.pack "")) VarDeclNoIdentifier
    _ <- symbolP SymEq <|> registerError (withDummyPos ()) VarDeclNoEquals
    expr <- exprP <|> customFailure VarDeclNoExpression
    (_,_,endPos) <- symbolP SymSemicolon <|>
        registerError (withDummyPos ()) (NoClosingDelimiter SymSemicolon)
    pure $ VarDecl (Loc startPos endPos) mty ident expr
\end{minted}
%
The local \code{varOrTypeP} helper function (\haskell{where}-clause omitted
here) parses either the \spl{var} keyword or a monomorphic type, returning an
instance of \haskell{Maybe Type} along with a \haskell{SourcePos}.
In the next two lines, if we cannot parse the identifier or the `\spl{=}' symbol,
we skip them and register a corresponding error message. The most
likely scenario is that they are missing altogether; in that case, we can
provide further useful error messages if we just continue parsing the remainder
of the declaration.

Next, our statement parser is defined as follows:
\begin{minted}{haskell}
  stmtP :: TokenParser Stmt
  stmtP = ifP <|> whileP <|> returnP <|> try assignP <|> funCallSP
\end{minted}
%
If-else-blocks, while-loops and return statements all begin with a distinct
keyword token, meaning we can simply try them one after the other.
Assignments and function calls both start with an identifier though, hence we
need to wrap the \code{assignP} with \code{try}. The resulting parser will
attempt to parse an assignment, but backtracks upon failure, after which we try
to parse a function call instead.
The else-block of an if-statement is optional, and our parser allows it to be
omitted, in which case we specify the else-statements as the empty list.

The parsers for function declarations and statements use the error recovery
described in \cref{sec:parser-error-handling}, as illustrated by the parser for
while-loops:
\begin{minted}{haskell}
  whileP :: TokenParser Stmt
  whileP = do
    (_,start,_) <- keywordP KwWhile
    (cond,_,_) <- parensP exprP
    (stmts,_,end) <- bracesP $ many (withRecovery parseToNextStmt stmtP)
    pure $ While (Loc start end) cond stmts
\end{minted}
%
Once we have parsed the condition---using \code{parensP} for the parentheses and
\code{exprP} for the expression---we try to parse a sequence of statements
between braces `\verb|{...}|' using \code{parseToNextStmt} for
error recovery.

Finally, to illustrate the parsing of expressions discussed in
\cref{sec:precedence-associativity}, consider the parser for the $\NT{Expr}$
non-terminal, as defined in the grammar of \cref{fig:new-expr-grammar}:
%
\begin{minted}{haskell}
  exprP :: TokenParser Expr
  exprP = propP >>= opPropP
    where
      opP :: TokenParser BinaryOp
      opP = (symbolP SymAndAnd >> pure And) <|> (symbolP SymPipePipe >> pure Or)
      opPropP :: Expr -> TokenParser Expr
      opPropP prop = (try opP >>= \op -> do
        prop' <- propP <|> registerError GarbageExpr (BinaryOpNoExpression op)
        opPropP (BinOp (Loc (getStart prop) (getEnd prop')) op prop prop')) <|> pure prop
\end{minted}
%
We start by parsing an instance of $\NT{Prop}$, i.e. of the following precedence
level. The result is then passed (monadically with \haskell{>>=}) to
\code{opPropP}, which tries to parse one of the binary operators \spl{&&} or
\code{||}, followed by another $\NT{Prop}$.
If we fail to parse an operator and second operand, we just return the
result of \code{propP}.

Note in particular how the parameter of \code{opPropP} serves to
\emph{accumulate} the subsequent operands one-by-one. This way, we can
construct the desired left-deep syntax tree, as outlined in
\cref{sec:precedence-associativity}.
Parsing the whole remaining subterm after the operator recursively with
\code{exprP} would result in a right-deep nesting, which does not match the
desired associativity for \spl{&&} and \code{||}.
We instead parse only the next operand using \code{propP}, and then call
\code{opPropP} recursively, where the argument is given by the \haskell{BinOp}
node featuring the (old) accumulator and the new operand.

The remaining expression parsers for binary operations all work in the same way.
For unary operations and atoms, the parser functions are much as expected, again
following the transformed grammar of \cref{fig:new-expr-grammar}.
Much like the examples shown so far, most parser functions include some form of
error correction, where we allow the parser to skip certain components once we
are on a `fixed path', e.g. if an operator is not followed by an
operand, or if a period `\code{.}' is not followed by a field selector.

One final aspect worth mentioning is how our parser for $\NT{Atom}$ deals
with tuples \code{($e$,$e'$)} and parenthesis pairs \code{($e$)}.
We could naively try to parse one option first, i.e. consume
the entire expression $e$ until we reach the `\code{,}' or `\code{)}'.
If we picked the wrong case, we then need to backtrack to the start of $e$,
which could be very costly.
To avoid backtracking, we use the following approach instead:
\begin{minted}{haskell}
  parenOrTupleP :: TokenParser Expr
  parenOrTupleP = do
    (_,start,_) <- symbolP SymParenLeft
    e <- exprP <|> registerError GarbageExpr ParenOpenNoExpression
    closeExprP e <|> closeTupleP e start
\end{minted}
%
We first parse the opening parenthesis `\code{(}' and the expression $e$, which
both paths have in common.
We then pass \code{e} first to the helper function
\code{closeExprP}, which tries to parse a closing parenthesis `\code{)}'.
If that parser fails, we try to parse the remainder of a tuple using
\code{closeTupleP} instead, i.e. a comma, second expression $e'$, and then a
closing parenthesis.
Since we have already parsed to the end of $e$, \code{closeExprP} fails
immediately if the next token is a `\code{,}', and we avoid any backtracking.


% \begin{todoenv}
%   \begin{itemize}
%     \item How did you design the Abstract Syntax Tree
%     \item How does the parser work?
%     \item How did you handle difficult things like fixity, associativity etc.
%     \item Is there error handling? Recovery?
%     \item Do you have a lexer and parser?
%     \item How do they communicate?
%     \item Problems?
%   \end{itemize}
% \end{todoenv}
