In this chapter we discuss the lexing and parsing phases of our compiler.
For both these phases we make use of the megaparsec
library~\footnote{\url{https://hackage.haskell.org/package/megaparsec}}.

\section{Lexing}
Our compiler has separate lexing and parsing phases.
This is mainly to make the work of the parser easier.
Firstly, the lexer takes care of skipping whitespace tokens.
This also includes skipping single and multiline comments.
Second, it also annotates all tokens with their position in the file.
The lexer returns a list of \mintinline{haskell}{Positioned Token},
where \mintinline{haskell}{Positioned a} is defined as follows:
\begin{minted}{haskell}
data Positioned a = Positioned
  { startPos :: SourcePos,
    endPos :: SourcePos,
    startOffset :: Int,
    tokenLength :: Int,
    tokenVal :: a
  }

data SourcePos = SourcePos
  { sourceName :: FilePath,
    sourceLine :: !Pos,
    sourceColumn :: !Pos
  }
\end{minted}
This will later make it possible to provide good error message to the user,
as they will be able to locate the error using the source positions.

Our parsing library of choice, Megaparsec, makes dealing with whitespace quite
straightforward. The library allows one to define a \emph{space consumer}, which
can consume spaces, tabs, line breaks as well as line and block comments,
according to delimiters specified by the client.
By defining the delimiters \code{//} as well as \code{/*} and \code{*/}, we can
instruct Megaparsec to strip all line and block comments from the input.
For block comments, Megaparsec even offers an option for handling nested block
comments.

The lexer has error recovery, which is quite easy to implement.
There are basically two types of errors in our lexer.
Either the lexer encounters an invalid integer (e.g. \verb|123ab|),
or it encounters an unknown symbol.
In both cases we recover from the error by repeatedly discarding characters from
the input until we can successfully lex a token again.


\section{Parsing}
In order to write parser functions corresponding to the provided grammar of SPL,
we needed to transform the grammar in order to eliminate occurrences of left
recursion and to obtain the desired structure of the AST.
The main challenge when implementing the parser was the parsing of expressions.
In particular, the rule
\[ \NT{Exp} \Coloneqq \NT{Exp}\ \NT{Op2}\ \NT{Exp} \]
is left-recursive and must be rewritten.
Additionally, when parsing expressions of this form, we must consider the
associativity and precedence of the operators when designing the rules of our
parser grammar in order to obtain the desired AST.

We achieved this by creating a recursive descent parser with multiple levels,
corresponding to the precedence levels of the operators.
The parser thus begins at the lowest precedence level, featuring the Boolean
operators \code{\&\&} and \code{||}.
The leftmost non-terminal in the first rule is $\NT{Prop}$ and not $\NT{Expr}$
itself, allowing us to avoid infinite parsing loops caused by left recursion.

In order to still parse arbitrarily many applications of \code{\&\&} or
\code{||}, we use the pattern $((\code{\&\&} \mid \code{||})\ \NT{Prop})^*$,
hence we can parse any expression of the form $\dots \code{\&\&} \dots \code{||} \dots$,
and also simply a $\NT{Prop}$, since $*$ denotes zero or more occurrences of
a pattern.

For the following rules of the expression grammar, we proceed in much the same
manner: They consist of an expression matching the following rule, followed
by zero or more occurrences of a binary operator and another expression matching
the following rule. The resulting expression grammar after applying the
transformations outlined above is shown in \cref{fig:new-expr-grammar}.

The desired AST for the expression
\[ \code{2 == 1+1 \&\& 2 != 3-2} \]
is a conjunction of two (in)equalities, rather than (for instance) an equality
between \code{2} and the remaining conjunction.
Our parser should yield the desired AST without requiring the programmer to add
parentheses, hence the precedence of \code{\&\&} must be lower than that of
\code{==} and \code{!=}.

\begin{figure}[h]
  \[
  \begin{array}{lcl}
		%
		\NT{Expr} & \Coloneqq & \NT{Prop}\ ((\code{\&\&} \mid \code{||})\ \NT{Prop})^* \br
		%
		\NT{Prop} & \Coloneqq & \NT{List}\ ((\code{==} \mid \code{!=} \mid \code{<} \mid \code{>} \mid \code{<=} \mid \code{>=})\ \NT{List})^* \br
		%
		\NT{List} & \Coloneqq & \NT{Form}\ (\code{:}\ \NT{Form})^* \br
		%
		\NT{Form} & \Coloneqq & \NT{Term}\ (( \code{+} \mid \code{-} )\ \NT{Term})^* \br
		%
		\NT{Term} & \Coloneqq & \NT{Val}\ (( \code{*} \mid \code{/} \mid \code{\%} )\ \NT{Val} )^* \br
		%
		\NT{Val} & \Coloneqq & \code{(}\ \NT{Expr}\ \code{)} \mid \code{(}\ \NT{Expr}\ \code{,}\ \NT{Expr}\ \code{)} \\
		& \mid & \NT{FunCall} \mid \NT{Id} \NT{Field} \mid \NT{Int} \mid \NT{Bool} \mid \NT{Char} \\
		& \mid & \code{[]} \mid \code{!}\ \NT{Val} \mid \code{-}\ \NT{Val}
		%
  \end{array}
  \]
  \caption{Parser grammar for expressions}
  \label{fig:new-expr-grammar}
\end{figure}

Similarly, all comparison operators should have a lower precedence than the
\emph{cons} operator ``\code{:}'', which in turn should be followed by addition
and subtraction, then by multiplication, division and modulo and finally by
Boolean negation and unary minus.
Following this precedence hierarchy, we end up with the multiple rules or
\emph{levels} of the grammar in \cref{fig:new-expr-grammar}.


Another important consideration for the expression grammar is the
\emph{associativity} of the various operators. For instance, since addition
associates to the left, we want the expression \code{1+2+3} to result in an AST
of the form \code{Add (Add 1 2) 3} rather than \code{Add 1 (Add 2 3)} (see
\cref{fig:ast} for the actual AST data type of our implementation).
In fact, all binary operators included in SPL are typically left-associative, in
contrast to e.g. exponentiation, which is usually taken to be right-associative.

We obtain the correct AST through the grammar rule pattern
$\NT{e}\ (\NT{op}\ \NT{e})^*$ discussed previously.
Here, we first parse an instance of $\NT{e}$, which we then (possibly) combine
repeatedly with another instance of $\NT{e}$. This way, we can use an
accumulator $a$, which we initialise with the leftmost subexpression
$e_1$, then replace by $a = op\ e_1\ e_2$, then to $a = op\ a\ e_3$, and so
forth. By successively combining the (nested) expression parsed thus far with
the following subexpression, we achieve the desired left-deep nesting of the
AST.


\section{Abstract Syntax Tree}
\textbf{TODO: Update AST if we update code}
\begin{figure}
\begin{minted}[breaklines]{haskell}
data Program = Program [VarDecl] [FunDecl]
data VarDecl = VarDecl (Maybe Type) T.Text Expr
data FunDecl = FunDecl T.Text [T.Text] (Maybe Type) [VarDecl] [Stmt]

data Stmt = If Expr [Stmt] [Stmt]
          | While Expr [Stmt]
          | Assign Field Expr
          | FunCall T.Text [Expr]
          | Return (Maybe Expr)
          | GarbageS

data Type = IntT
          | BoolT
          | CharT
          | Prod Type Type
          | List Type
          | Void
          | Fun [Type] Type
          | TyVar T.Text
          | GarbageT

data Field = Ident T.Text
           | Head Field
           | Tail Field
           | Fst Field
           | Snd Field

data Expr = Field Field
          | Int Integer
          | Char Char
          | Bool Bool
          | UnOp UnaryOp Expr
          | BinOp BinaryOp Expr Expr
          | FunCallE T.Text [Expr]
          | EmptyList
          | Tuple Expr Expr

data UnaryOp = Not | Neg

data BinaryOp = Add | Sub | Mul | Div | Mod | Eq | Neq | Lt | Gt | Lteq | Gteq | And | Or | Cons
\end{minted}
	\caption{Abstract syntax tree}\label{fig:ast}
\end{figure}
Our syntax tree can found in Figure~\ref{fig:ast}.
Our top level node is \mintinline{haskell}{Program}.
A \mintinline{haskell}{Program} consists of a list of variable declarations,
and a list of function declarations.
One thing to note is that we have some garbage AST nodes, such as \mintinline{haskell}{GarbageS} and \mintinline{haskell}{GarbageT}.
These are used to support some sort of error recovery.
Using these garbage nodes we can still produce a valid AST,
such that the parser can recover and continue parsing.
It is essential that programs with these nodes will not be type checked.
After the parsing phase, the compiler should print the errors and stop compilation.
We can thus threat it as a compiler bug, if we come to a later phase and encounter these garbage tokens.

To model field lookups we use the \mintinline{haskell}{Field} datatype,
where a field \mintinline{haskell}{Snd (Fst "x")} should be interpreted as \lstinline[language=SPL]|x.fst.snd|.

We did not create a special data type for built-in functions like \lstinline[language=SPL]|print|, \lstinline[language=SPL]|isEmpty|, etc.
The main reason for this, is that we suspect that it will be easier to treat these as regular functions,
where there is already an implementation for them.
However, it may be the case that in the future we find that it is easier to create a special datatype for built-in functions,
which means we have to change it later.

We also did not add source position information to the AST yet.
We will do this whenever we find we need this information in the AST.

\begin{itemize}
	\item How did you design the Abstract Syntax Tree
	\item How does the parser work?
	\item How did you handle difficult things like fixity, associativity etc.
	\item Is there error handling? Recovery?
	\item Do you have a lexer and parser?
	\item How do they communicate?
	\item Problems?
	\item\ldots
\end{itemize}
