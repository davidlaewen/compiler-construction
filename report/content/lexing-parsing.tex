\chapter{Lexing \& Parsing}

In this chapter we discuss the lexing and parsing phases of our compiler.
For both these phases we make use of Haskell's \texttt{megaparsec}
library~\footnote{\url{https://hackage.haskell.org/package/megaparsec}}.

\section{Lexing}
Our compiler has separate lexing and parsing phases.
This is mainly to make the work of the parser easier, thanks to the preprocessing
performed by the lexer.
Firstly, the lexer takes care of skipping whitespace tokens.
This also includes skipping single and multiline comments.
Second, it also annotates all tokens with their position in the file.
The lexer returns a list of \mintinline{haskell}{Positioned Token},
where \mintinline{haskell}{Positioned a} is defined as follows:
\begin{minted}{haskell}
data Positioned a = Positioned
  { startPos :: SourcePos,
    endPos :: SourcePos,
    startOffset :: Int,
    tokenLength :: Int,
    tokenVal :: a
  }

data SourcePos = SourcePos
  { sourceName :: FilePath,
    sourceLine :: !Pos,
    sourceColumn :: !Pos
  }
\end{minted}
This will later make it possible to provide more informative error messages, as
we can include the file, line and column range in which the error occurs,
allowing the user to easily locate the error in their source code.

Our parsing library of choice, Megaparsec, makes dealing with whitespace quite
straightforward. The library allows one to define a \emph{space consumer}, which
can consume spaces, tabs, line breaks as well as line and block comments,
according to delimiters specified by the client.
By defining the delimiters \code{//} as well as \code{/*} and \code{*/}, we can
instruct Megaparsec to strip all line and block comments from the input.
For block comments, Megaparsec even offers an option for handling nested block
comments.

The lexer has error recovery, which is quite easy to implement.
There are basically two types of errors in our lexer.
Either the lexer encounters an invalid integer (e.g. \verb|123ab|),
or it encounters an unknown symbol.
In both cases we recover from the error by repeatedly discarding characters from
the input until we can successfully lex a token again.


\section{Parsing}
In order to write parser functions corresponding to the provided grammar of SPL,
we needed to transform the grammar in order to eliminate occurrences of left
recursion and to obtain the desired structure of the AST.
The main challenge when implementing the parser was the parsing of expressions.
In particular, the rule
\[ \NT{Exp} \Coloneqq \NT{Exp}\ \NT{Op2}\ \NT{Exp} \]
is left-recursive and must be rewritten.
Additionally, when parsing expressions of this form, we must consider the
associativity and precedence of the operators when designing the rules of our
parser grammar in order to obtain the desired AST.

We achieved this by creating a recursive descent parser with multiple levels,
corresponding to the precedence levels of the operators.
The parser thus begins at the lowest precedence level, featuring the Boolean
operators \code{\&\&} and \code{||}.
The leftmost non-terminal in the first rule is $\NT{Prop}$ and not $\NT{Expr}$
itself, allowing us to avoid infinite parsing loops caused by left recursion.

In order to still parse arbitrarily many applications of \code{\&\&} or
\code{||}, we use the pattern $((\code{\&\&} \mid \code{||})\ \NT{Prop})^*$,
hence we can parse any expression of the form $\dots \code{\&\&} \dots \code{||} \dots$,
and also simply a $\NT{Prop}$, since $*$ denotes zero or more occurrences of
a pattern.

For the following rules of the expression grammar, we proceed in much the same
manner: They consist of an expression matching the following rule, followed
by zero or more occurrences of a binary operator and another expression matching
the following rule. The resulting expression grammar after applying the
transformations outlined above is shown in \cref{fig:new-expr-grammar}.

The desired AST for the expression
\[ \code{2 == 1+1 \&\& 2 != 3-2} \]
is a conjunction of two (in)equalities, rather than (for instance) an equality
between \code{2} and the remaining conjunction.
Our parser should yield the desired AST without requiring the programmer to add
parentheses, hence the precedence of \code{\&\&} must be lower than that of
\code{==} and \code{!=}.

\begin{figure}[h]
  \[
  \begin{array}{lcl}
		%
		\NT{Expr} & \Coloneqq & \NT{Prop}\ ((\code{\&\&} \mid \code{||})\ \NT{Prop})^* \br
		%
		\NT{Prop} & \Coloneqq & \NT{List}\ ((\code{==} \mid \code{!=} \mid \code{<} \mid \code{>} \mid \code{<=} \mid \code{>=})\ \NT{List})^* \br
		%
		\NT{List} & \Coloneqq & \NT{Form}\ (\code{:}\ \NT{Form})^* \br
		%
		\NT{Form} & \Coloneqq & \NT{Term}\ (( \code{+} \mid \code{-} )\ \NT{Term})^* \br
		%
		\NT{Term} & \Coloneqq & \NT{Val}\ (( \code{*} \mid \code{/} \mid \code{\%} )\ \NT{Val} )^* \br
		%
		\NT{Val} & \Coloneqq & \code{(}\ \NT{Expr}\ \code{)} \ruleSep
      \code{(}\ \NT{Expr}\ \code{,}\ \NT{Expr}\ \code{)} \\
		& \mid & \NT{FunCall} \ruleSep
      \NT{Id} \NT{Field} \\
    & \mid & \NT{Int} \ruleSep \NT{Bool} \ruleSep \NT{Char} \\
		& \mid & \code{!}\ \NT{Val} \ruleSep \code{-}\ \NT{Val} \ruleSep \code{[]}
		%
  \end{array}
  \]
  \caption{Parser grammar for expressions}
  \label{fig:new-expr-grammar}
\end{figure}

Similarly, all comparison operators should have a lower precedence than the
\emph{cons} operator ``\code{:}'', which in turn should be followed by addition
and subtraction, then by multiplication, division and modulo and finally by
Boolean negation and unary minus.
Following this precedence hierarchy, we end up with the multiple rules or
\emph{levels} of the grammar in \cref{fig:new-expr-grammar}.

Another important consideration for the expression grammar is the
\emph{associativity} of the various operators. For instance, since addition
associates to the left, we want the expression \code{1+2+3} to result in an AST
of the form \code{Add (Add 1 2) 3} rather than \code{Add 1 (Add 2 3)} (see
\cref{fig:ast} for the actual AST data type of our implementation).
In fact, all binary operators included in SPL are typically left-associative, in
contrast to e.g. exponentiation, which is usually taken to be right-associative.

We obtain the correct AST through the grammar rule pattern
$\NT{e}\ (\NT{op}\ \NT{e})^*$ discussed previously.
Here, we first parse an instance of $\NT{e}$, which we then (possibly) combine
repeatedly with another instance of $\NT{e}$. This way, we can use an
accumulator $a$, which we initialise with the leftmost subexpression
$e_1$, then replace by $a = op\ e_1\ e_2$, then to $a = op\ a\ e_3$, and so
forth. By successively combining the (nested) expression parsed thus far with
the following subexpression, we achieve the desired left-deep nesting of the
AST.

Certain terminals and non-terminals, such as $\NT{Char}$ or \code{[]}, could be
placed higher up in the grammar, since characters and the empty list cannot be
used in arithmetic operations. Despite this, we chose to place these in the
$\NT{Val}$ rule, since it is more intuitive to provide a type error rather than
a parse error for non-numbers occuring in an arithmetic expression.


\section{Abstract Syntax Tree}

\begin{figure}
\begin{minted}[breaklines]{haskell}
data Program = Program [VarDecl] [FunMutDecl]
data VarDecl = VarDecl Loc (Maybe Type) T.Text Expr
data FunDecl = FunDecl Loc T.Text [T.Text] (Maybe Type) [VarDecl] [Stmt]
data FunMutDecl = MutualDecls Loc [FunDecl] | SingleDecl FunDecl

data Stmt = If Loc Expr [Stmt] [Stmt]
          | While Loc Expr [Stmt]
          | Assign Loc VarLookup Expr
          | FunCall Loc T.Text [Expr]
          | Return Loc (Maybe Expr)
          | GarbageStmt

data Type = IntT Loc
          | BoolT Loc
          | CharT Loc
          | Prod Loc Type Type
          | List Loc Type
          | Void Loc
          | Fun Loc [Type] Type
          | TyVar Loc T.Text
          | GarbageType

data VarLookup = VarId Loc T.Text | VarField Loc VarLookup Field
data ExprLookup = ExprField Expr Field
data Field = Head | Tail | Fst | Snd | GarbageField

data Expr = Ident Loc T.Text
          | ExprLookup Loc ExprLookup
          | Int Loc Int
          | Bool Loc Bool
          | Char Loc Char
          | UnOp Loc UnaryOp Expr
          | BinOp Loc BinaryOp Expr Expr
          | FunCallE Loc T.Text [Expr]
          | EmptyList Loc
          | Tuple Loc Expr Expr
          | GarbageExpr

data UnaryOp = Not | Neg
data BinaryOp = Add | Sub | Mul | Div | Mod | Eq | Neq | Lt | Gt | Lte | Gte | And | Or | Cons
\end{minted}
	\caption{Abstract syntax tree for parsing stage}
  \label{fig:parse-ast}
\end{figure}

Our syntax tree can be found in \cref{fig:parse-ast}. Our top level node is
\haskell{Program}. A \haskell{Program} consists of a
list of variable declarations, and a list of function declarations.

One thing to note is that at each parser level, we include a `garbage' AST node,
such as \haskell{GarbageStmt} for statements, or \haskell{GarbageExpr} for
expressions. In the case of a parse error, these nodes allow us to construct a
valid AST and continue our parsing pass, while also clearly marking the AST as
erroneous. When we later translate our parse AST to the AST representation used
for typing, these nodes prevent us from accidentally entering the typing stage
when parse errors where present in the source program, which would clearly
constitute a compiler bug.
Since the `garbage' nodes allow us to continue parsing after we encounter a
parse error, they are particularly useful for supporting multiple error messages
and error recovery.

To model field lookups we use the \haskell{Field} datatype, where a field
\haskell{Snd (Fst "x")} should be interpreted as \spl{x.fst.snd}.
Field selectors can appear both on the left side of assignments, e.g.
\spl{x.fst = 42;}, and in expressions, e.g. \spl{(0,40+2).snd}. In both cases, we
model the field selectors using \haskell{Field}, which features cases for the
built-in list and tuple projections.
In both variable assignments and expressions, field selectors can be chained
arbitrarily, e.g. \spl{x.tl.tl.hd.fst}, for which we use recursion in the AST
definition.
Variables in variable assignments are modelled using \haskell{VarLookup}, an
instance of which is either an identifier or a field lookup. The latter case is
self-referential, as it consists of a field lookup on instance of
\haskell{VarLookup} itself.
Similarly, \haskell{ExprLookup} is mutually recursive with \haskell{Expr}, so in
a field lookup of the form $e.s$, $e$ can be an arbitrary expression, so can
again be of the form $e.s$, and so on.

We did not create a special data type for built-in functions like
\spl{print} or \spl{isEmpty}, since we currently treat them as regular functions
of a fixed type and predetermined implementation, which is either hardcoded in
the code generation, or given in a prelude file that is always loaded by the
compiler.




We also did not add source position information to the AST yet.
We will do this whenever we find we need this information in the AST.

\begin{itemize}
	\item How did you design the Abstract Syntax Tree
	\item How does the parser work?
	\item How did you handle difficult things like fixity, associativity etc.
	\item Is there error handling? Recovery?
	\item Do you have a lexer and parser?
	\item How do they communicate?
	\item Problems?
\end{itemize}
