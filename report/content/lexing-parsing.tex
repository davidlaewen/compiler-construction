\chapter{Lexing \& Parsing} \label{chp:lexing-parsing}

In this chapter we discuss the lexing and parsing phases of our compiler.
For both these phases we make use of the Megaparsec
library\footnote{\url{https://hackage.haskell.org/package/megaparsec}} for Haskell.
%
Our compiler has two separate phases for lexing and parsing.
This is mainly to make the work of the parser easier through the preprocessing
performed by the lexer.
The two phases also constitute a separation of concerns, where the lexer
tokenises the input, converting e.g. literals and keywords into corresponding
tokens, while the parser deals with the program structure and the construction
of the AST from the token stream returned by the lexer.

\section{Lexing}
Firstly, the lexer deals with stripping whitespace and comments, which includes
skipping single and multiline comments.
Secondly, this stage tokenises the input, converting the source file into a
stream of tokens.
%
Tokens are one of either\vspace{-1mm}
\begin{itemize}[itemsep=0mm]
  \item integer, Boolean, or character literals;
  \item keywords and symbols, as defined in our keyword and symbol table; or
  \item tokens for function/variable identifiers.
\end{itemize}
%
The above is captured by the following data type:
\begin{minted}{haskell}
  data Token = IntLit Int | BoolLit Bool | CharLit Char
             | IdToken T.Text
             | NameToken T.Text
             | Symbol Symbol
             | Keyword Keyword
    deriving (Eq, Ord)
\end{minted}


\paragraph{Literals}
The lexer tokenises integer, Boolean and character literals, converting them to
instances of \haskell{IntLit Int}, \haskell{BoolLit Bool} or \haskell{CharLit Char},
respectively.
For integers, we attempt to read in a sequence of decimal characters,
for Booleans we attempt to read one of the strings \haskell{"True"} or
\haskell{"False"}, and for character literals we attempt to read an alphanumeric
character, space or punctuation symbol between two single quotation marks
\haskell{"\'"}.

In each case, we require that the sequence is not followed by further
alphanumeric characters or underscores (or just letters, in the case of
integers), so e.g. the Boolean lexer would succeed if the next character is
non-alphanumeric, e.g. \spl{True;} or \spl{True&}, but would fail on \spl{True1}
or \spl{True_}, thereby allowing identifiers (for which lexing is attempted
later) to have \spl{True} or \spl{False} as a prefix.


\paragraph{Keywords and symbols}
All keywords and symbols are enumerated in the data types \haskell{Keyword} and
\haskell{Symbol}, where \haskell{Keyword} includes the statement keywords
\spl{if}, \spl{else}, \spl{while}, \spl{return}, the variable declaration
keyword \spl{var}, the type names \spl{Int}, \spl{Bool}, \spl{Char}, \spl{Void},
and the field selectors \spl{hd}, \spl{tl}, \spl{fst}, and \spl{snd}.

The symbols cover all the unary and binary operators, including those consisting
of two characters, such as \spl{\|\|}, \spl{&&}, \spl{=}\spl{=}, \spl{!=}, and
other reserved symbols such as \spl{::} and \spl{->} for type signatures, or
\spl{=}, \spl{;}, \spl{\{} and \spl{\}} for statements and code blocks.
%
\begin{minted}{haskell}
  data Keyword = KwVar | KwIf | KwElse | ... | KwFst | KwSnd

  instance Show Keyword where
    show KwVar     = "var"
    show KwIf      = "if"
    show KwElse    = "else"
    ...
    show KwFst     = "fst"
    show KwSnd     = "snd"

  data Symbol = SymColonColon | SymRightArrow | ... | SymBraceLeft | SymBraceRight

  instance Show Symbol where
  show SymColonColon    = "::"
  show SymRightArrow    = "->"
  ...
  show SymBraceLeft     = "{"
  show SymBraceRight    = "}"
\end{minted}

We can then use the Enum library to obtain a list of all keywords/symbols
instances in the data type, with which we can then define the corresponding
lexer, e.g. in the case of symbols:
%
\begin{minted}{haskell}
  symbols :: [Symbol]
  symbols = enumFromTo minBound maxBound

  symbolL :: Lexer Token
  symbolL = choice $ (\sym -> string (T.pack (show sym)) >> pure (Symbol sym)) <$> symbols
\end{minted}
%
The \code{choice} function will try the alternatives in the given list of lexers
in order, returning immediately if one of them succeeds, and failing otherwise.
For each symbol \code{sym}, the list we supply to \code{choice} includes a lexer
that attempts to lex the output of \code{show sym}, and returns the token
\haskell{Symbol sym} if it succeeds.

Defining the symbol and keyword lexer in this way allows us to modify the
symbols and keywords by simply updating the data type declarations and show
instances, without needing to touch any code in the actual lexer.
A slight caveat is that we need to pay attention to the order in which we
declare the constructors of \haskell{Symbol} when
symbols\footnote{The same is true for keywords, but our SPL implementation does not feature any keywords with
overlapping prefixes.} have overlapping prefixes, as is the case for e.g.
\spl{:} and \spl{::}, or \spl{[} and \spl{[]} (the empty list).
We then need to ensure that we attempt to lex the longer symbol before its
prefix, which we achieve by declaring it earlier in the data type definition.


\paragraph{Identifiers}
The last alternative in the lexer is that for identifiers, which can be any
letter character\footnote{We actually reserve uppercase names for data constructors,
further details on which we defer to \cref{chp:extension}.}
followed by a sequence of alphanumeric characters or underscores.
Identifiers cover global and local variable names, function names, and
function parameter names.

\begin{minted}{haskell}
  identL :: Lexer Token
  identL = do
    name <- T.cons <$> letterChar <*> (T.pack <$> many (alphaNumChar <|> char '_'))
    pure $ IdToken name
\end{minted}


\paragraph{Source locations}
During tokenisation, the lexer also annotates all tokens with their position in
the file. The lexer returns a list of \mintinline{haskell}{Positioned Token},
where \mintinline{haskell}{Positioned a} is defined as follows:
%
\begin{minted}{haskell}
  data Positioned a = Positioned {
    startPos :: SourcePos,
    endPos :: SourcePos,
    startOffset :: Int,
    tokenLength :: Int,
    tokenVal :: a
  }

  data SourcePos = SourcePos {
    sourceName :: FilePath,
    sourceLine :: !Pos,
    sourceColumn :: !Pos
  }
\end{minted}

These source locations are propagated through the stages of our compiler,
allowing us to specify source code locations in error messages throughout the
compilation process, which remain accurate even after translating our AST
representation in the desugaring phase (\cref{sec:desugaring}).
%
All user errors result in error messages featuring a file name together with a
line and column range, indicating the part of the program from which the error
arose. Depending on the shell/terminal, these links are even clickable, allowing
the user to immediately jump to the location of the error in their source code.

\paragraph{Whitespace and comments}
Our parsing library of choice, Megaparsec, makes dealing with whitespace quite
straightforward. The library allows one to define a \emph{space consumer}, which
can consume spaces, tabs, line breaks, line comments and block comments
according to delimiters specified by the client.
By defining the delimiters \code{//} as well as \code{/*} and \code{*/}, we can
instruct Megaparsec to strip all line and block comments from the input.
Megaparsec even features a built-in option for correctly
handling nested block comments.

\paragraph{Error recovery}
The lexer has error recovery, which is quite easy to implement.
There are essentially two types of errors in our lexer:
either the lexer encounters an invalid integer (e.g. \verb|123ab|),
or it encounters an unknown symbol not featured in the symbol table.
In both cases we recover from the error by repeatedly discarding characters from
the input until we can successfully lex a token again.


\section{Parsing}

\subsection{Operator Precedence \& Associativity}
In order to write parser functions corresponding to the provided grammar of SPL,
we needed to transform the grammar such that we eliminate occurrences of left
recursion and obtain the desired structure of the AST with respect to
associativity and operator precedence.
This pertains especially to the parsing of expressions, where the rule
\[ \NT{Exp} \Coloneqq \NT{Exp}\ \NT{Op2}\ \NT{Exp} \]
is left-recursive and must be rewritten.
In addition, it seems desirable to assign precedence levels for the various
binary operators supported by SPL, and treat them as either left-associative or
right-associative, depending on the operator.
We must take this into consideration when designing the rules of our parser
grammar, which we use as the basis for defining our monadic parser combinators.

To illustrate, let us consider what AST our parser should generate for the
expression
\[ \code{2 == 1+1 \&\& 2 != 3-2} .\]
One would expect this to denote a conjunction of two (in)equalities, rather than
(for instance) an equality between \code{2} and everything right of the
`\code{==}', or an inequality between `\code{2 == 1+1 \&\& 2}' and `\code{3-2}'.
%
The latter two options of course make no sense, but more importantly, they do
not match the operator precedence typically assumed in written formulas or other
programming languages, so the programmer would not expect the expression to be
parsed as such.

Ideally, our parser should yield the desired AST without requiring the
programmer to add parentheses, hence the precedence of \code{\&\&} must be lower
than that of \code{==} and \code{!=}.
Similarly, all comparison operators should have a lower precedence than the
\emph{cons} operator ``\code{:}'', which in turn should be followed by addition
and subtraction, then by multiplication, division and modulo and finally by
Boolean negation and unary minus.

We achieve this behaviour using a recursive descent parser with multiple levels,
shown in \cref{fig:new-expr-grammar}, where each non-terminal corresponds to a
level in the precedence hierarchy of operators.
%
The parser thus begins at the lowest precedence level, featuring the Boolean
operators \code{\&\&} and \code{||}.
In the rule for $\NT{Expr}$, the leftmost non-terminal is $\NT{Prop}$ and not
$\NT{Expr}$ itself, allowing us to avoid infinite parsing loops caused by left
recursion.
%
\begin{figure}[t]
  \[
  \begin{array}{lcl}
		%
		\NT{Expr} & \Coloneqq & \NT{Prop}\ ((\code{\&\&} \mid \code{||})\ \NT{Prop})^* \br
		%
		\NT{Prop} & \Coloneqq & \NT{List}\ ((\code{==} \mid \code{!=} \mid \code{<} \mid \code{>} \mid \code{<=} \mid \code{>=})\ \NT{List})^* \br
		%
		\NT{List} & \Coloneqq & \NT{Form}\ (\code{:}\ \NT{Form})^* \br
		%
		\NT{Form} & \Coloneqq & \NT{Term}\ (( \code{+} \mid \code{-} )\ \NT{Term})^* \br
		%
		\NT{Term} & \Coloneqq & \NT{Val}\ (( \code{*} \mid \code{/} \mid \code{\%} )\ \NT{Val} )^* \br
		%
		\NT{Val} & \Coloneqq & \code{(}\ \NT{Expr}\ \code{)} \ruleSep
      \code{(}\ \NT{Expr}\ \code{,}\ \NT{Expr}\ \code{)} \\
		& \mid & \NT{FunCall} \ruleSep
      \NT{Id} \NT{Field} \\
    & \mid & \NT{Int} \ruleSep \NT{Bool} \ruleSep \NT{Char} \\
		& \mid & \code{!}\ \NT{Val} \ruleSep \code{-}\ \NT{Val} \ruleSep \code{[]}
		%
  \end{array}
  \]
  \caption{Parser grammar for expressions}
  \label{fig:new-expr-grammar}
\end{figure}
%
Below the expression level, denoted by $\NT{Expr}$, our grammar
features the following levels, in increasing precedence level (where some level
names are rather arbitrary):
\begin{itemize}
  \item Propositions $\NT{Prop}$, which include the comparison operators;
  \item $\NT{List}$, with the \spl{:} (cons) operator on lists;
  \item formulas $\NT{Form}$, featuring addition and subtraction;
  \item terms $\NT{Term}$, featuring multiplication, division, and the modulo
        operator;
  \item and finally values $\NT{Val}$, which include integer, Boolean and
        character literals, the empty list \spl{[]} the unary operators \spl{!}
        and \spl{-}, function calls, identifiers, tuples, or just a pair of
        parentheses, which modifies the precedence.
\end{itemize}

The rule for $\NT{Val}$ allows us to jump back to $\NT{Expr}$ at the topmost
level of the grammar, either via an opening parenthesis \spl{(}, or the
arguments of a function call \code{$f$($e_1$,$\dots$,$e_n$)}.

In order to still parse arbitrarily many applications of \code{\&\&} or
\code{||}, we use the pattern $((\code{\&\&} \mid \code{||})\ \NT{Prop})^*$,
hence we can parse any expression of the form $\dots \code{\&\&} \dots \code{||} \dots$,
or just a single $\NT{Prop}$, since $*$ denotes zero or more occurrences of
a pattern.

For the following rules of the expression grammar, we proceed in much the same
manner: They consist of an expression matching the following rule, followed
by zero or more occurrences of a binary operator and another expression matching
the following rule. The resulting expression grammar after applying the
transformations outlined above is shown in \cref{fig:new-expr-grammar}.


Another important consideration for the expression grammar is the
\emph{associativity} of the various operators. For instance, since addition
associates to the left, we want the expression \code{1+2+3} to result in an AST
of the form \code{Add (Add 1 2) 3} rather than \code{Add 1 (Add 2 3)} (see
\cref{fig:parse-ast} for the actual AST data type of our implementation).
In fact, all binary operators included in SPL are typically left-associative, in
contrast to e.g. exponentiation, which is usually taken to be right-associative.

We obtain the correct AST through the grammar rule pattern
$\NT{e}\ (\NT{op}\ \NT{e})^*$ discussed previously.
Here, we first parse an instance of $\NT{e}$, which we then optionally combine
repeatedly with another instance of $\NT{e}$. We can do so using an
accumulator $a$, which we initialise with the leftmost subexpression
$e_1$, then update by $a \leftarrow op\ e_1\ e_2$, then by
$a \leftarrow op\ a\ e_3$, and so forth. By successively combining the (nested)
expression parsed thus far with the following subexpression, we achieve the
desired left-deep nesting of the AST.

Certain terminals and non-terminals, such as $\NT{Char}$ or \code{[]}, could be
placed higher up in the grammar, since e.g. characters and the empty list cannot
be used in arithmetic operations. Despite this, we chose to place these in the
$\NT{Val}$ rule, since it is more intuitive to provide a type error rather than
a parse error for non-numbers occuring in an arithmetic expression.


\section{Abstract Syntax Tree}

\begin{figure}
\begin{minted}[breaklines]{haskell}
data Program = Program [VarDecl] [FunMutDecl]
data VarDecl = VarDecl Loc (Maybe Type) T.Text Expr
data FunDecl = FunDecl Loc T.Text [T.Text] (Maybe Type) [VarDecl] [Stmt]
data FunMutDecl = MutualDecls Loc [FunDecl] | SingleDecl FunDecl

data Stmt = If Loc Expr [Stmt] [Stmt]
          | While Loc Expr [Stmt]
          | Assign Loc VarLookup Expr
          | FunCall Loc T.Text [Expr]
          | Return Loc (Maybe Expr)
          | GarbageStmt

data Type = IntT Loc
          | BoolT Loc
          | CharT Loc
          | Prod Loc Type Type
          | List Loc Type
          | Void Loc
          | Fun Loc [Type] Type
          | TyVar Loc T.Text
          | GarbageType

data VarLookup = VarId Loc T.Text | VarField Loc VarLookup Field
data ExprLookup = ExprField Expr Field
data Field = Head | Tail | Fst | Snd | GarbageField

data Expr = Ident Loc T.Text
          | ExprLookup Loc ExprLookup
          | Int Loc Int
          | Bool Loc Bool
          | Char Loc Char
          | UnOp Loc UnaryOp Expr
          | BinOp Loc BinaryOp Expr Expr
          | FunCallE Loc T.Text [Expr]
          | EmptyList Loc
          | Tuple Loc Expr Expr
          | GarbageExpr

data UnaryOp = Not | Neg
data BinaryOp = Add | Sub | Mul | Div | Mod | Eq | Neq | Lt | Gt | Lte | Gte | And | Or | Cons
\end{minted}
	\caption{Abstract syntax tree for parsing stage}
  \label{fig:parse-ast}
\end{figure}

Our syntax tree can be found in \cref{fig:parse-ast}. Our top level node is
\haskell{Program}. A \haskell{Program} consists of a
list of variable declarations, and a list of function declarations.

At each parser level, we include a `garbage' AST node,
such as \haskell{GarbageStmt} for statements, or \haskell{GarbageExpr} for
expressions. In the case of a parse error, these nodes allow us to construct a
valid AST and continue our parsing pass, while also clearly marking the AST as
erroneous. When we later translate our parse AST to the AST representation used
for typing, these nodes prevent us from accidentally entering the typing stage
when parse errors where present in the source program, which would clearly
constitute a compiler bug.
Since the `garbage' nodes allow us to continue parsing after we encounter a
parse error, they are particularly useful for supporting multiple error messages
and error recovery.

To model field lookups we use the \haskell{Field} datatype, where a field
\haskell{Snd (Fst "x")} should be interpreted as \spl{x.fst.snd}.
Field selectors can appear both on the left side of assignments, e.g.
\spl{x.fst = 42;}, and in expressions, e.g. \spl{(0,40+2).snd}. In both cases, we
model the field selectors using \haskell{Field}, which features cases for the
built-in list and tuple projections.
In both variable assignments and expressions, field selectors can be chained
arbitrarily, e.g. \spl{x.tl.tl.hd.fst}, for which we use recursion in the AST
definition.
Variables in variable assignments are modelled using \haskell{VarLookup}, an
instance of which is either an identifier or a field lookup. The latter case is
self-referential, as it consists of a field lookup on instance of
\haskell{VarLookup} itself.
Similarly, \haskell{ExprLookup} is mutually recursive with \haskell{Expr}, so in
a field lookup of the form $e.s$, $e$ can be an arbitrary expression, so can
again be of the form $e.s$, and so on.

We did not create a special data type for built-in functions like
\spl{print} or \spl{isEmpty}, since we currently treat them as regular functions
of a fixed type and predetermined implementation, which is either hardcoded in
the code generation, or given in a prelude file that is always loaded by the
compiler.




We also did not add source position information to the AST yet.
We will do this whenever we find we need this information in the AST.

\begin{todoenv}
  \begin{itemize}
    \item How did you design the Abstract Syntax Tree
    \item How does the parser work?
    \item How did you handle difficult things like fixity, associativity etc.
    \item Is there error handling? Recovery?
    \item Do you have a lexer and parser?
    \item How do they communicate?
    \item Problems?
  \end{itemize}
\end{todoenv}
