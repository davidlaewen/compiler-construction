\chapter{Lexing \& Parsing} \label{chp:lexing-parsing}

In this chapter we discuss the lexing and parsing phases of our compiler.
For both phases we make use of the Megaparsec
library\footnote{\url{https://hackage.haskell.org/package/megaparsec}}.
%
Our compiler has two separate phases for lexing and parsing.
This is mainly to make the work of the parser easier through the preprocessing
performed by the lexer.
The two phases also constitute a separation of concerns: the lexer
tokenises the input, converting e.g. literals and keywords into corresponding
tokens, and handling whitespace and comments; the parser deals with the actual
program structure and the construction of the AST from the token stream returned
by the lexer.
%
As a consequence, we require an additional pass for lexing the input prior to
parsing, and the lexer cannot leverage the program structure to e.g. exclude
certain cases or provide more informative error messages.
However, as we will see, the parser benefits in many ways from this
preprocessing.

We first cover the lexing stage in \cref{sec:lexing}, after which we discuss
the setup of parsing stage in \cref{sec:parsing-setup}, our parse AST
representation in \cref{sec:parse-ast}, and the parsing of programs in
\cref{sec:parsing-programs}.


\section{Lexing} \label{sec:lexing}

The lexer has two main functionalities.
Firstly, it deals with stripping whitespace and comments, which includes
skipping single-line and multi-line comments.
Secondly, this stage tokenises the text input, converting the source file into a
stream of tokens.

Our lexer is implemented using Megaparsec, and has the following type:
\begin{minted}{haskell}
  type Lexer a = Parsec Void Text a
\end{minted}
%
The type of the input stream is \haskell{Text}. We use \haskell{Void} for the
custom error type, as Megaparsec's built-in error type is sufficient for
handling the few forms of error we might encounter during lexing.
%
Our lexer instances are generally of type \haskell{Lexer Token}, that is, they
try to consume part of the input, returning a token if they succeed.

Tokens are one of either \vspace{-1mm}
\begin{itemize}[itemsep=0mm]
  \item integer, Boolean, or character literals;
  \item keywords and symbols, as defined in our keyword and symbol table; or
  \item tokens for function/variable identifiers.
\end{itemize}
%
The above is captured by the following data type:
\begin{minted}{haskell}
  data Token = IntLit Int | BoolLit Bool | CharLit Char
             | IdToken T.Text
             | NameToken T.Text
             | Symbol Symbol
             | Keyword Keyword
    deriving (Eq, Ord)
\end{minted}


\paragraph{Literals}
The lexer tokenises integer, Boolean and character literals, converting them to
instances of \haskell{IntLit Int}, \haskell{BoolLit Bool} or \haskell{CharLit Char},
respectively.
For integers, we attempt to read in a sequence of decimal characters,
for Booleans we attempt to read one of the strings \haskell{"True"} or
\haskell{"False"}, and for character literals we attempt to read an alphanumeric
character, space or punctuation symbol between two single quotation marks
\haskell{"\'"}.

In each case, we require that the sequence is not followed by further
alphanumeric characters or underscores (or just letters, in the case of
integers), so e.g. the Boolean lexer would succeed if the next character is
non-alphanumeric, e.g. \spl{True;} or \spl{True&}, but would fail on \spl{True1}
or \spl{True_}, thereby allowing identifiers (for which lexing is attempted
later) to have \spl{True} or \spl{False} as a prefix.


\paragraph{Keywords and symbols}
All keywords and symbols are enumerated in the data types \haskell{Keyword} and
\haskell{Symbol}, where \haskell{Keyword} includes the statement keywords
\spl{if}, \spl{else}, \spl{while}, \spl{return}, the variable declaration
keyword \spl{var}, the type names \spl{Int}, \spl{Bool}, \spl{Char}, \spl{Void},
and the field selectors \spl{hd}, \spl{tl}, \spl{fst}, and \spl{snd}.

The symbols cover all the unary and binary operators, including those consisting
of two characters, such as \spl{\|\|}, \spl{&&}, \spl{=}\spl{=}, \spl{!=}, and
other reserved symbols such as \spl{::} and \spl{->} for type signatures, or
\spl{=}, \spl{;}, \spl{\{} and \spl{\}} for statements and code blocks.
%
\begin{minted}{haskell}
  data Keyword = KwVar | KwIf | KwElse | ... | KwFst | KwSnd

  instance Show Keyword where
    show KwVar     = "var"
    show KwIf      = "if"
    show KwElse    = "else"
    ...
    show KwFst     = "fst"
    show KwSnd     = "snd"

  data Symbol = SymColonColon | SymRightArrow | ... | SymBraceLeft | SymBraceRight

  instance Show Symbol where
  show SymColonColon    = "::"
  show SymRightArrow    = "->"
  ...
  show SymBraceLeft     = "{"
  show SymBraceRight    = "}"
\end{minted}

We can then use the Enum library to obtain a list of all keywords/symbols
instances in the data type, with which we can then define the corresponding
lexer, e.g. in the case of symbols:
%
\begin{minted}{haskell}
  symbols :: [Symbol]
  symbols = enumFromTo minBound maxBound

  symbolL :: Lexer Token
  symbolL = choice $ (\sym -> string (T.pack (show sym)) >> pure (Symbol sym)) <$> symbols
\end{minted}
%
The \code{choice} function will try the alternatives in the given list of lexers
in order, returning immediately if one of them succeeds, and failing otherwise.
For each symbol \code{sym}, the list we supply to \code{choice} includes a lexer
that attempts to lex the output of \code{show sym}, and returns the token
\haskell{Symbol sym} if it succeeds.

Defining the symbol and keyword lexer in this way allows us to modify the
symbols and keywords by simply updating the data type declarations and show
instances, without needing to touch any code in the actual lexer.
A slight caveat is that we need to pay attention to the order in which we
declare the constructors of \haskell{Symbol} when
symbols have overlapping prefixes, as is the case for e.g.
\spl{:} and \spl{::}, or \spl{[} and \spl{[]} (the empty list).\footnote{The
same is true for keywords, but our SPL implementation does not feature any
keywords with overlapping prefixes.}
We then need to ensure that we attempt to lex the longer symbol before its
prefix, which we achieve by declaring it earlier in the data type definition.


\paragraph{Identifiers}
The last alternative in the lexer is that for identifiers, which can be any
letter character\footnote{We actually reserve capitalised names for data
constructors, further details on which we defer to \cref{chp:extension}.}
followed by a sequence of alphanumeric characters or underscores.
Identifiers cover global and local variable names, function names, and
function parameter names.

\begin{minted}{haskell}
  identL :: Lexer Token
  identL = do
    name <- T.cons <$> letterChar <*> (T.pack <$> many (alphaNumChar <|> char '_'))
    pure $ IdToken name
\end{minted}


\paragraph{Source locations}
During tokenisation, the lexer also annotates all tokens with their position in
the file. The lexer returns a list of \mintinline{haskell}{Positioned Token},
where \mintinline{haskell}{Positioned a} is defined as follows:
%
\begin{minted}{haskell}
  data Positioned a = Positioned {
    startPos :: SourcePos,
    endPos :: SourcePos,
    startOffset :: Int,
    tokenLength :: Int,
    tokenVal :: a
  }

  data SourcePos = SourcePos {
    sourceName :: FilePath,
    sourceLine :: !Pos,
    sourceColumn :: !Pos
  }
\end{minted}

These source locations are propagated through the stages of our compiler,
allowing us to specify source code locations in error messages throughout the
compilation process, which remain accurate even after translating our AST
representation in the desugaring phase (\cref{sec:desugaring}).
%
All user errors result in error messages featuring a file name together with a
line and column range, indicating the part of the program from which the error
arose. If supported by the shell/terminal (e.g. that of VSCode), these links are
even clickable, allowing the user to immediately jump to the location of the
error in their source code.

\paragraph{Whitespace and comments}
Our parsing library of choice, Megaparsec, makes dealing with whitespace quite
straightforward. The library allows one to define a \emph{space consumer}, which
can consume spaces, tabs, line breaks, line comments and block comments
according to delimiters specified by the client.
By defining the delimiters \code{//} as well as \code{/*} and \code{*/}, we can
instruct Megaparsec to strip all line and block comments from the input.
Megaparsec even features a built-in option for correctly
handling nested block comments.

\paragraph{Error recovery}
The lexer supports error recovery, which is quite simple to achieve.
There are essentially just two types of errors that can occur while lexing:
either we encounter an invalid integer (e.g. \verb|123ab|),
or we encounter a symbol not featured in the symbol table.
In both cases we recover from the error by repeatedly discarding characters from
the input until we can successfully lex a token again.
In any case, we fail after the lexing stage and do not continue to parsing, but
the lexer error recovery allows us to identify multiple errors in one lexing
pass and generate error messages for each of them.


\section{Parsing Stage Setup} \label{sec:parsing-setup}

Before we discuss the AST representation in \cref{sec:parse-ast} and the actual
parsing of programs in \cref{sec:parsing-programs}, we give a more technical
account of the underlying `machinery' of the parsing stage in the present
section. In particular, we cover the custom token stream type on which our
parser operates, how we define custom parse errors, and support for error
recovery and reporting multiple errors.

While \haskell{Lexer} operates on input of type \haskell{Data.Text}, our parser
\haskell{TokenParser} operates on the token stream returned by the lexer.
The type of our parser is given by:
%
\begin{minted}{haskell}
  type TokenParser a = Parsec ParserError TokenStream a
\end{minted}
Here, \haskell{TokenStream} specifies the type of the input stream,
\haskell{ParserError} is the type of our custom error component, and the type
variable \code{a} (made explicit above by eta-expansion) specifies the output
type of the parser instance.


\subsection{Custom Token Stream}

Our lexer instances generally return a single token, that is, an instance of
the \haskell{Token} data type shown in \cref{sec:lexing}. By lexing many such
tokens from the input, we obtain a list of tokens, hence the input of type
\haskell{Data.Text} is processed into a list \haskell{[Positioned Token]},
where \haskell{Positioned a} equips the underlying type \code{a} with positional
information:
\begin{minted}{haskell}
  data Positioned a = Positioned {
    startPosition :: SourcePos,
    endPosition   :: SourcePos,
    startOffset   :: Int,
    tokenLength   :: Int,
    tokenVal      :: a
  } deriving (Eq, Ord, Show)
\end{minted}

In order to use Megaparsec with our custom token type, we need to make our list
of tokens an instance of Megaparsec's \haskell{Stream} type class\footnote{\url{https://hackage.haskell.org/package/megaparsec-9.7.0/docs/Text-Megaparsec-Stream.html}}.
Simply instantiating \haskell{Stream} with \haskell{[Positioned Token]} is not
sufficient though, since we want our parser to display segments or lines from
the original input in our error messages, which is not possible based on the
token stream alone.
To this end, we must bundle the list of tokens returned by the lexer with the
original source program of type \haskell{Data.Text}:
%
\begin{minted}{haskell}
  data TokenStream = TokenStream {
    tokenStreamInput :: T.Text,
    tokenStream :: [Positioned PT.Token]
  } deriving (Show)
\end{minted}

We give type class declarations of \haskell{TokenStream} for the \haskell{Stream}
type class, along with the \haskell{VisualStream} and \haskell{TraversableStream}
type classes, which are needed for error reporting and debugging support.
This involves a fair amount of function definitions, though most of the
implementation closely follows the example code given in the Megaparsec tutorial
of Mark Karpov, which discusses the use of Megaparsec with custom input
streams\footnote{\url{https://markkarpov.com/tutorial/megaparsec.html\#working-with-custom-input-streams}}.

Since this is really just boilerplate code, we will forego a detailed
explanation, but to summarise briefly, we need to define a number of functions
for decomposing a \haskell{TokenStream} instance into a (list of)
\haskell{Positioned Token} and the remaining stream, along with functions for
displaying a \haskell{TokenStream} instance as a string, and a function that
takes an offset in the input and returns the corresponding line and updates the
positional state of the stream, shifting it to the position given by the offset.


\subsection{Custom Parse Errors}

Megaparsec can generate so-called trivial parse errors out of the box, where the
error message displays the position of the parse error and the corresponding
line from the input, listing the unexpected token that was encountered (if any),
and the tokens that were expected.
We can additionally generate `fancy' custom parse errors. A naive method for
doing so is using the \haskell{fail} combinator, which always fails, reporting
its string argument as the error.

A cleaner approach is to use Megaparsec's support for extending its
\haskell{ErrorFancy} type by embedding a custom data type of parse errors.
We use this capability to define a number of custom errors for various scenarios,
with a few example cases shown below:
%
\begin{minted}{haskell}
  data ParserError where
    NoClosingDelimiter   :: PT.Symbol -> ParserError
    FunctionNoArrow      :: ParserError
    ProdTypeMissingEntry :: ParserError
    ...
    IfNoCondition        :: ParserError
\end{minted}

We can then specify how our custom errors should be displayed by defining a
corresponding type class instance for Megaparsec's \haskell{ShowErrorComponent}:
%
\begin{minted}{haskell}
  instance ShowErrorComponent ParserError where
    showErrorComponent :: ParserError -> String
    showErrorComponent (NoClosingDelimiter sym) = "Expecting closing `" <> show sym <> "`"
    showErrorComponent FunctionNoArrow = "Expecting argument type or `->`"
    showErrorComponent ProdTypeMissingEntry = "Expecting left type of product"
    ...
    showErrorComponent IfNoCondition = "Expecting if condition between `(` and `)`"
\end{minted}

Instances of \haskell{ParserError} are used throughout our parser implementation
to provide more informative error messages for many possible issues we might
encounter while parsing the input.
Using a data type to do so allows us to reuse our custom errors in different
parts of the parser, and ensures that the various forms of error are always
displayed in a consistent format.


\todobox{Describe error recovery}


\section{Parse AST Representation} \label{sec:parse-ast}

\begin{figure}
\begin{minted}[breaklines]{haskell}
data Program = Program [VarDecl] [FunMutDecl]
data VarDecl = VarDecl Loc (Maybe Type) T.Text Expr
data FunDecl = FunDecl Loc T.Text [T.Text] (Maybe Type) [VarDecl] [Stmt]
data FunMutDecl = MutualDecls Loc [FunDecl] | SingleDecl FunDecl

data Stmt = If Loc Expr [Stmt] [Stmt]
          | While Loc Expr [Stmt]
          | Assign Loc VarLookup Expr
          | FunCall Loc T.Text [Expr]
          | Return Loc (Maybe Expr)
          | GarbageStmt

data Type = IntT Loc | BoolT Loc | CharT Loc
          | Prod Loc Type Type | List Loc Type | Void Loc
          | Fun Loc [Type] Type
          | TyVar Loc T.Text
          | GarbageType

data VarLookup = VarId Loc T.Text | VarField Loc VarLookup Field
data ExprLookup = ExprField Expr Field
data Field = Head | Tail | Fst | Snd | GarbageField

data Expr = Ident Loc T.Text
          | ExprLookup Loc ExprLookup
          | Int Loc Int
          | Bool Loc Bool
          | Char Loc Char
          | UnOp Loc UnaryOp Expr
          | BinOp Loc BinaryOp Expr Expr
          | FunCallE Loc T.Text [Expr]
          | EmptyList Loc
          | Tuple Loc Expr Expr
          | GarbageExpr

data UnaryOp = Not | Neg
data BinaryOp = Add | Sub | Mul | Div | Mod | Eq | Neq | Lt | Gt | Lte | Gte
              | And | Or | Cons
\end{minted}
	\caption{Abstract syntax tree for parsing stage}
  \label{fig:parse-ast}
\end{figure}

The data types of our AST representation can be found in \cref{fig:parse-ast}.
Our top level node is \haskell{Program}. A \haskell{Program} consists of a list
of variable declarations, and a list of function declarations.

At each parser level, we include a `garbage' AST node,
such as \haskell{GarbageStmt} for statements, or \haskell{GarbageExpr} for
expressions. In the case of a parse error, these nodes allow us to construct a
valid AST and continue our parsing pass, while also clearly marking the AST as
erroneous. When we later translate our parse AST to the AST representation used
for typing, these nodes prevent us from accidentally entering the typing stage
when parse errors were present in the source program, which would clearly
constitute a compiler bug.
Since the `garbage' nodes allow us to continue parsing after we encounter a
parse error, they are particularly useful for supporting multiple error messages
and error recovery.

To model field lookups we use the \haskell{Field} datatype, where a field
\haskell{Snd (Fst "x")} should be interpreted as \spl{x.fst.snd}.
Field selectors can appear both on the left side of assignments, e.g.
\spl{x.fst = 42;}, and in expressions, e.g. \spl{(0,40+2).snd}. In both cases, we
model the field selectors using \haskell{Field}, which features cases for the
built-in list and tuple projections.
In both variable assignments and expressions, field selectors can be chained
arbitrarily, e.g. \spl{x.tl.tl.hd.fst}, for which we use recursion in the AST
definition.
Variables in variable assignments are modelled using \haskell{VarLookup}, an
instance of which is either an identifier or a field lookup. The latter case is
self-referential, as it consists of a field lookup on instance of
\haskell{VarLookup} itself.
Similarly, \haskell{ExprLookup} is mutually recursive with \haskell{Expr}, so in
a field lookup of the form $e.s$, $e$ can be an arbitrary expression, so can
again be of the form $e.s$, and so on.

We did not create a special data type for built-in functions like
\spl{print} or \spl{isEmpty}, since we currently treat them as regular functions
of a fixed type and predetermined implementation, which is either hardcoded in
the code generation, or given in a prelude file that is always loaded by the
compiler.




\section{Parsing of Programs} \label{sec:parsing-programs}

\subsection{Operator Precedence \& Associativity}

In order to write parser functions corresponding to the provided grammar of SPL,
we needed to transform the grammar such that we eliminate occurrences of left
recursion and obtain the desired structure of the AST with respect to
associativity and operator precedence.
This pertains especially to the parsing of expressions, where the rule
\[ \NT{Exp} \Coloneqq \NT{Exp}\ \NT{Op2}\ \NT{Exp} \]
is left-recursive and must be rewritten.
In addition, it seems desirable to assign precedence levels for the various
binary operators supported by SPL, and treat them as either left-associative or
right-associative, depending on the operator.
We must take this into consideration when designing the rules of our parser
grammar, which we use as the basis for defining our monadic parser combinators.

To illustrate, let us consider what AST our parser should generate for the
expression
\[ \code{2 == 1+1 \&\& 2 != 3-2} .\]
One would expect this to denote a conjunction of two (in)equalities, rather than
(for instance) an equality between \code{2} and everything right of the
`\code{==}', or an inequality between `\code{2 == 1+1 \&\& 2}' and `\code{3-2}'.
%
The latter two options of course make no sense, but more importantly, they do
not match the operator precedence typically assumed in written formulas or other
programming languages, so the programmer would not expect the expression to be
parsed as such.

Ideally, our parser should yield the desired AST without requiring the
programmer to add parentheses, hence the precedence of \code{\&\&} must be lower
than that of \code{==} and \code{!=}.
Similarly, all comparison operators should have a lower precedence than the
\emph{cons} operator ``\code{:}'', which in turn should be followed by addition
and subtraction, then by multiplication, division and modulo and finally by
Boolean negation and unary minus.

We achieve this behaviour using a recursive descent parser with multiple levels,
shown in \cref{fig:new-expr-grammar}, where each non-terminal corresponds to a
level in the precedence hierarchy of operators.
%
The parser thus begins at the lowest precedence level, featuring the Boolean
operators \code{\&\&} and \code{||}.
In the rule for $\NT{Expr}$, the leftmost non-terminal is $\NT{Prop}$ and not
$\NT{Expr}$ itself, allowing us to avoid infinite parsing loops caused by left
recursion.
%
Below the expression level, denoted by $\NT{Expr}$, our grammar
features the following levels, in increasing precedence level (where some level
names are rather arbitrary):
\begin{itemize}
  \item Propositions $\NT{Prop}$, which include the comparison operators;
  \item $\NT{List}$, with the \spl{:} (cons) operator on lists;
  \item formulas $\NT{Form}$, featuring addition and subtraction;
  \item terms $\NT{Term}$, featuring multiplication, division, and the modulo
        operator;
  \item and finally values $\NT{Val}$, which include integer, Boolean and
        character literals, the empty list \spl{[]}, the unary operators \spl{!}
        and \spl{-}, function calls, identifiers, tuples, or just a pair of
        parentheses, which modify the precedence.
\end{itemize}

\begin{figure}[th]
  \[
  \begin{array}{lcl}
		%
		\NT{Expr} & \Coloneqq & \NT{Prop}\ ((\code{\&\&} \mid \code{||})\ \NT{Prop})^* \br
		%
		\NT{Prop} & \Coloneqq & \NT{List}\ ((\code{==} \mid \code{!=} \mid \code{<} \mid \code{>} \mid \code{<=} \mid \code{>=})\ \NT{List})^* \br
		%
		\NT{List} & \Coloneqq & \NT{Form}\ (\code{:}\ \NT{Form})^* \br
		%
		\NT{Form} & \Coloneqq & \NT{Term}\ (( \code{+} \mid \code{-} )\ \NT{Term})^* \br
		%
		\NT{Term} & \Coloneqq & \NT{Val}\ (( \code{*} \mid \code{/} \mid \code{\%} )\ \NT{Val} )^* \br
		%
		\NT{Val} & \Coloneqq & \code{(}\ \NT{Expr}\ \code{)} \ruleSep
      \code{(}\ \NT{Expr}\ \code{,}\ \NT{Expr}\ \code{)} \\
		& \mid & \NT{FunCall} \ruleSep
      \NT{Id} \NT{Field} \\
    & \mid & \NT{Int} \ruleSep \NT{Bool} \ruleSep \NT{Char} \\
		& \mid & \code{!}\ \NT{Val} \ruleSep \code{-}\ \NT{Val} \ruleSep \code{[]}
		%
  \end{array}
  \]
  \caption{Parser grammar for expressions}
  \label{fig:new-expr-grammar}
\end{figure}
%

The rule for $\NT{Val}$ allows us to jump back to $\NT{Expr}$ at the topmost
level of the grammar, either via an opening parenthesis \spl{(}, or the
arguments of a function call \code{$f$($e_1$,$\dots$,$e_n$)}.

In order to still parse arbitrarily many applications of \code{\&\&} or
\code{||}, we use the pattern $((\code{\&\&} \mid \code{||})\ \NT{Prop})^*$,
hence we can parse any expression of the form $\dots \code{\&\&} \dots \code{||} \dots$,
or just a single $\NT{Prop}$, since $*$ denotes zero or more occurrences of
a pattern.

For the following rules of the expression grammar, we proceed in much the same
manner: They consist of an expression matching the following rule, followed
by zero or more occurrences of a binary operator and another expression matching
the following rule. The resulting expression grammar after applying the
transformations outlined above is shown in \cref{fig:new-expr-grammar}.


Another important consideration for the expression grammar is the
\emph{associativity} of the various operators. For instance, since addition
associates to the left, we want the expression \code{1+2+3} to result in an AST
of the form \code{Add (Add 1 2) 3} rather than \code{Add 1 (Add 2 3)} (see
\cref{fig:parse-ast} for the actual AST data type of our implementation).
In fact, all binary operators included in SPL are typically left-associative, in
contrast to e.g. exponentiation, which is usually taken to be right-associative.

We obtain the correct AST through the grammar rule pattern
$\NT{e}\ (\NT{op}\ \NT{e})^*$ discussed previously.
Here, we first parse an instance of $\NT{e}$, which we then optionally combine
repeatedly with another instance of $\NT{e}$. We can do so using an
accumulator $a$, which we initialise with the leftmost subexpression
$e_1$, then update by $a \leftarrow op\ e_1\ e_2$, then by
$a \leftarrow op\ a\ e_3$, and so forth. By successively combining the (nested)
expression parsed thus far with the following subexpression, we achieve the
desired left-deep nesting of the AST.

Certain terminals and non-terminals, such as $\NT{Char}$ or \code{[]}, could be
placed higher up in the grammar, since e.g. characters and the empty list cannot
be used in arithmetic operations. Despite this, we chose to place these in the
$\NT{Val}$ rule, since it is more intuitive to provide a type error rather than
a parse error for non-numbers occurring in an arithmetic expression.




\begin{todoenv}
  \begin{itemize}
    \item How did you design the Abstract Syntax Tree
    \item How does the parser work?
    \item How did you handle difficult things like fixity, associativity etc.
    \item Is there error handling? Recovery?
    \item Do you have a lexer and parser?
    \item How do they communicate?
    \item Problems?
  \end{itemize}
\end{todoenv}
